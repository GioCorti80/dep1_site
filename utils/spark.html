<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>Spark Tips</h1>
<h3>Config</h3>
<pre>
from pyspark.sql import SparkSession

# Create Spark Session with increased memory
spark = SparkSession.builder \
        .appName("Avoid Heap Memory Error") \
        .config("spark.driver.memory", "4g") \  # Increase driver memory
        .config("spark.executor.memory", "4g") \  # Increase executor memory
        .config("spark.executor.memoryOverhead", "1g") \  # Additional overhead memory
        .config("spark.sql.shuffle.partitions", "200") \  # Reduce shuffle partitions if needed
        .getOrCreate()
</pre>
<pre>
.withColumn("col1", F.format_string("%.0f", F.col("col1").cast("double"))) \
                 .withColumn("col2", F.format_string("%.0f", F.col("col2").cast("double")))
</pre>
<h3>Set temporary gcs bucket</h3>
<pre>
    from pyspark.sql import SparkSession

    # Initialize SparkSession with BigQuery connector and GCS bucket configuration
    spark = SparkSession.builder \
        .appName("WriteToBigQuery") \
        .config("spark.jars.packages", "com.google.cloud:spark-bigquery-connector:0.34.0") \
        .config("spark.hadoop.google.cloud.bigquery.connector.temporaryGcsBucket", "your-temp-gcs-bucket") \
        .getOrCreate()
</pre>
<pre>
# Convert rows to a list of tuples
list_of_tuples = [tuple(row) for row in df.collect()]
print(list_of_tuples)
</pre>
<pre>
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Define schema
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])

# Create empty DataFrame
empty_df = spark.createDataFrame([], schema)

# Show result
empty_df.printSchema()
empty_df.show()

</pre>
<pre>
df = df.withColumn("size_gb", col("size_bytes") / 1073741824)
</pre>