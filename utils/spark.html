<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>Spark Tips</h1>
<h3>Config</h3>
<pre>
from pyspark.sql import SparkSession

# Create Spark Session with increased memory
spark = SparkSession.builder \
        .appName("Avoid Heap Memory Error") \
        .config("spark.driver.memory", "4g") \  # Increase driver memory
        .config("spark.executor.memory", "4g") \  # Increase executor memory
        .config("spark.executor.memoryOverhead", "1g") \  # Additional overhead memory
        .config("spark.sql.shuffle.partitions", "200") \  # Reduce shuffle partitions if needed
        .getOrCreate()
</pre>
<pre>
.withColumn("col1", F.format_string("%.0f", F.col("col1").cast("double"))) \
                 .withColumn("col2", F.format_string("%.0f", F.col("col2").cast("double")))
</pre>
<h3>Set temporary gcs bucket</h3>
<pre>
    from pyspark.sql import SparkSession

    # Initialize SparkSession with BigQuery connector and GCS bucket configuration
    spark = SparkSession.builder \
        .appName("WriteToBigQuery") \
        .config("spark.jars.packages", "com.google.cloud:spark-bigquery-connector:0.34.0") \
        .config("spark.hadoop.google.cloud.bigquery.connector.temporaryGcsBucket", "your-temp-gcs-bucket") \
        .getOrCreate()
</pre>
<pre>
# Convert rows to a list of tuples
list_of_tuples = [tuple(row) for row in df.collect()]
print(list_of_tuples)
</pre>
<pre>
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Initialize Spark
spark = SparkSession.builder.getOrCreate()

# Define schema
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)
])

# Create empty DataFrame
empty_df = spark.createDataFrame([], schema)

# Show result
empty_df.printSchema()
empty_df.show()

</pre>
<pre>
df = df.withColumn("size_gb", col("size_bytes") / 1073741824)
</pre>
<pre>
resource.type="dataproc_cluster"
resource.labels.cluster_name="your-cluster-name"
logName="projects/your-project-id/logs/dataproc.googleapis.com%2Fdriver"
severity>=ERROR
textPayload:"1234-job1"

</pre>
<pre>
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType
from datetime import date

# Create Spark session
spark = SparkSession.builder.appName("example").getOrCreate()

# Define schema
schema = StructType([
    StructField("col1", StringType(), True),
    StructField("col2", StringType(), True),
    StructField("col3_date", DateType(), True),
    StructField("col4_double", DoubleType(), True)
])

# Create data
data = [
    ("A1", "B1", date(2024, 7, 1), 10.5),
    ("A2", "B2", date(2024, 7, 2), 20.0),
    ("A3", "B3", None, None),  # This row has null for date and double
    ("A4", "B4", date(2024, 7, 4), 40.7)
]

# Create DataFrame
df = spark.createDataFrame(data, schema=schema)

# Show DataFrame
df.show()

df_with_age = df.withColumn(
    "age",
    floor(months_between(current_date(), col("date_of_birth")) / 12)
)

</pre>
<pre>
spark.read.option("encoding", "Cp280").csv("/path/to/output_cp280")

</pre>