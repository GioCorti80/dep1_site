<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>python Tips</h1>
<h3>Config</h3>
<pre>
list1 = ['a', 'b', 'c', 'd']
list2 = ['b', 'x', 'a', 'z', 'c']

filtered_list2 = [item in list1]
filtered_list2 = [item for item in list2 if item in list1]

print(filtered_list2)

</pre>
<pre>
from google.cloud.logging_v2.services.logging_service_v2 import LoggingServiceV2Client

</pre>
<pre>
from datetime import datetime, timedelta

project_id = "your-gcp-project-id"
log_filter = 'resource.type="cloud_function" AND severity>=ERROR'

# Example: Last 24 hours
start = datetime.utcnow() - timedelta(days=1)
end = datetime.utcnow()

logs = read_logs(project_id, log_filter, start, end)

for entry in logs:
    print(f"{entry.timestamp} - {entry.severity} - {entry.text_payload or entry.json_payload}")

</pre>
<pre>
import holidays
from datetime import date, datetime

def is_working_day(input_date=None, country='US'):
    """
    Checks if a given date (or today if not provided) is a working day.
    Excludes weekends and public holidays.

    Args:
        input_date (str or datetime.date or datetime.datetime, optional): 
            The date to check. If None, uses today. 
            If string, format should be 'YYYY-MM-DD'.
        country (str): Country code for holidays (default 'US').

    Returns:
        bool: True if it's a working day, False otherwise.
    """
    if input_date is None:
        check_date = date.today()
    elif isinstance(input_date, str):
        check_date = datetime.strptime(input_date, "%Y-%m-%d").date()
    elif isinstance(input_date, (date, datetime)):
        check_date = input_date.date() if isinstance(input_date, datetime) else input_date
    else:
        raise ValueError("Invalid date format. Use None, a 'YYYY-MM-DD' string, or a date object.")
    
    holiday_dates = holidays.country_holidays(country)
    
    return check_date.weekday() < 5 and check_date not in holiday_dates

</pre>
<pre>
df_with_new_date = df.withColumn("new_date", add_months(df["start_date"], 3))
</pre>
<pre>
import sys
print(f"Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}")
print("PySpark version:", spark.version)
</pre>
<pre>
import pandas as pd
import io
import sys

# Create a sample DataFrame
data = {
    'col1': [1, 2, 3, 4, 5],
    'col2': ['A', 'B', 'C', 'D', 'E'],
    'col3': [10.1, 20.2, None, 40.4, 50.5],
    'col4': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'])
}
df = pd.DataFrame(data)

# Specify the filename for the output
output_filename = "df_info_output.txt"

# Create a string buffer to capture the output
buffer = io.StringIO()

# Redirect stdout to the buffer
sys.stdout = buffer

# Call df.info() - its output will now go to the buffer
df.info()

# Restore stdout to its original state (the console)
sys.stdout = sys.__stdout__

# Get the captured output from the buffer
captured_output = buffer.getvalue()

# Write the captured output to a text file
with open(output_filename, 'w') as f:
    f.write(captured_output)

print(f"df.info() output saved to '{output_filename}'")

# You can also print the captured output to verify
# print("\n--- Captured Output ---")
# print(captured_output)
</pre>
<pre>
from datetime import datetime

filename = f"file_strs_{datetime.now().strftime('%Y_%m')}.ok"
print(filename)
</pre>
<pre>
from google.cloud import bigquery

# Initialize BigQuery client
client = bigquery.Client()

# Define your table in the format: project.dataset.table
table_id = "your-project.your_dataset.your_table"

# Get the table metadata
table = client.get_table(table_id)

# Get the last modified time (returns timestamp in milliseconds)
last_modified_time = table.modified  # This is a datetime.datetime object

print(f"Last modified time: {last_modified_time}")
</pre>
<pre>
from google.cloud import storage
from datetime import datetime

def get_latest_file_from_gcs(bucket_name, folder='', prefix='', extension=''):
    """
    Returns the latest file in a GCS bucket directory based on last modified time.

    Parameters:
    - bucket_name (str): Name of the GCS bucket.
    - folder (str): Folder path inside the bucket ('' means root). Should not start with '/'.
    - prefix (str): Prefix of the file name.
    - extension (str): File extension to filter (e.g. '.csv', '.json').

    Returns:
    - str: Full GCS path of the latest file (gs://bucket_name/path/to/file).
    """
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    # Normalize folder path
    folder_prefix = folder.strip('/')
    full_prefix = f"{folder_prefix}/{prefix}".lstrip('/') if folder_prefix else prefix

    blobs = client.list_blobs(bucket_name, prefix=full_prefix)

    # Filter by extension and track latest blob
    latest_blob = None
    for blob in blobs:
        if extension and not blob.name.endswith(extension):
            continue
        if latest_blob is None or blob.updated > latest_blob.updated:
            latest_blob = blob

    if latest_blob:
        return f"gs://{bucket_name}/{latest_blob.name}"
    else:
        return None

</pre>
<pre>
import re

text = """
Some SQL or text that mentions pwcc_dqm.tab1, pwcc_dqm.tab2, and maybe pwcc_dqm.tab3
even inside longer strings or comments like 'select * from pwcc_dqm.tab4'.
"""

# Regular expression to match pwcc_dqm.<table_name>
matches = re.findall(r'\bpwcc_dqm\.\w+\b', text)

</pre>
<pre>
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

# Initialize client
client = bigquery.Client()

# Define table ID: 'project.dataset.table'
table_id = "your-project.your_dataset.your_table"

try:
    client.get_table(table_id)  # API request
    print(f"✅ Table {table_id} exists.")
    table_exists = True
except NotFound:
    print(f"❌ Table {table_id} does not exist.")
    table_exists = False

bool_value = value.lower() == "true"
</pre>
<pre>
from datetime import datetime

# Get today's date
now = datetime.now()

# Format the date strings
date_str = now.strftime("%Y%m%d")
datetime_str = now.strftime("%Y%m%d%H%M%S")

# Build the filename
filename = f"125479-4_Log_auditing_{date_str}_{datetime_str}.csv"

print(filename)

</pre>
<pre>
import subprocess

result = subprocess.run(["ls", "-l"], capture_output=True, text=True)
print(result.stdout)

</pre>
<pre>
import requests
from requests.exceptions import RequestException
import os

def check_api_connection(api_url):
    """
    Checks if a public API endpoint is reachable using a HEAD request.

    :param api_url: The full URL of the public API endpoint to check.
    :return: True if the connection is successful, False otherwise.
    """
    # No headers are needed for public APIs that don't require authentication.
    print(f"Attempting to connect to: {api_url}")
    print("-" * 30)

    try:
        # Use requests.head() to send a HEAD request.
        # The timeout parameter prevents the script from hanging indefinitely.
        response = requests.head(api_url, timeout=5)

        # A status code in the 200 range (2xx) indicates success.
        if response.status_code >= 200 and response.status_code < 300:
            print("✅ Connection successful!")
            print(f"Status Code: {response.status_code}")
            return True
        else:
            print(f"❌ Connection failed. Status Code: {response.status_code}")
            # The 'reason' provides a human-readable explanation of the status code
            print(f"Reason: {response.reason}")
            return False

    except RequestException as e:
        # This catches any network-related errors (e.g., DNS failure, connection refused).
        print(f"❌ An error occurred: {e}")
        return False

# --- Main part of the script ---
if __name__ == "__main__":
    # This is a public API endpoint that doesn't require an API key.
    # It's a great choice for testing the script's functionality.
    PUBLIC_API_URL = "https://jsonplaceholder.typicode.com/posts/1"

    check_api_connection(PUBLIC_API_URL)
</pre>
<pre>
import requests
from requests.exceptions import RequestException
import os
import urllib3

# Disable SSL warnings for unverified HTTPS requests
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def check_api_connection(api_url):
    """
    Checks if a public API endpoint is reachable using a HEAD request.

    :param api_url: The full URL of the public API endpoint to check.
    :return: True if the connection is successful, False otherwise.
    """
    print(f"Attempting to connect to: {api_url}")
    print("-" * 30)

    try:
        # Add verify=False to skip SSL certificate verification
        response = requests.head(api_url, timeout=5, verify=False)

        if 200 <= response.status_code < 300:
            print("✅ Connection successful!")
            print(f"Status Code: {response.status_code}")
            return True
        else:
            print(f"❌ Connection failed. Status Code: {response.status_code}")
            print(f"Reason: {response.reason}")
            return False

    except RequestException as e:
        print(f"❌ An error occurred: {e}")
        return False

# --- Main part of the script ---
if __name__ == "__main__":
    PUBLIC_API_URL = "https://jsonplaceholder.typicode.com/posts/1"
    check_api_connection(PUBLIC_API_URL)
</pre>
<pre>
from datetime import date, timedelta
import calendar

# Get today's date
today = date.today()

# Get the first day of the current month
first_day_this_month = today.replace(day=1)

# Subtract one day → last day of previous month
last_day_prev_month = first_day_this_month - timedelta(days=1)

# Format as string
result = last_day_prev_month.strftime("%Y-%m-%d")
print(result)

</pre>
<pre>
from datetime import datetime

# Get the current date and time
current_date = datetime.now()

# Extract the month as a number
month_number = current_date.month

if month_number in [1,4,7,10]:
  print("trimestre")
else:
  print("mensile")
</pre>
<pre>
from datetime import datetime, timedelta

# Get the current date
current_date = datetime.now()

# Get the first day of the current month
first_day_of_current_month = current_date.replace(day=1)

# Subtract one day to get the last day of the previous month
last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)

# Format the date as a string in 'YYYY-MM-DD' format
last_day_str = last_day_of_previous_month.strftime('%Y-%m-%d')

# Print the final result
print(f"The last day of the previous month is: {last_day_str}")
</pre>
<pre>
import requests
from requests.exceptions import RequestException
import certifi

def check_api_connection(api_url):
    """
    Checks if a public API endpoint is reachable using a HEAD request.

    :param api_url: The full URL of the public API endpoint to check.
    :return: True if the connection is successful, False otherwise.
    """
    print(f"Attempting to connect to: {api_url}")
    print(f"Using certifi CA bundle: {certifi.where()}")
    print("-" * 30)

    try:
        # Use certifi's CA bundle for SSL verification
        response = requests.head(api_url, timeout=5, verify=certifi.where())

        if 200 <= response.status_code < 300:
            print("✅ Connection successful!")
            print(f"Status Code: {response.status_code}")
            return True
        else:
            print(f"❌ Connection failed. Status Code: {response.status_code}")
            print(f"Reason: {response.reason}")
            return False

    except RequestException as e:
        print(f"❌ An error occurred: {e}")
        return False

# --- Main part of the script ---
if __name__ == "__main__":
    PUBLIC_API_URL = "https://jsonplaceholder.typicode.com/posts/1"
    check_api_connection(PUBLIC_API_URL)

</pre>
<pre>
import ssl
import socket

def fetch_cert_chain(hostname, port=443, output_file="company_ca_bundle.pem"):
    # Create a connection and wrap with SSL to fetch certs
    ctx = ssl.create_default_context()
    conn = ctx.wrap_socket(socket.socket(socket.AF_INET), server_hostname=hostname)
    conn.connect((hostname, port))

    # Get the full certificate chain
    der_certs = conn.getpeercert(True)
    chain = conn.getpeercertchain() if hasattr(conn, "getpeercertchain") else [der_certs]

    conn.close()

    # Write the certs to a PEM file
    with open(output_file, "wb") as f:
        for cert in chain:
            pem = ssl.DER_cert_to_PEM_cert(cert)
            f.write(pem.encode("utf-8"))

    print(f"✅ Certificate chain for {hostname} saved to {output_file}")


if __name__ == "__main__":
    fetch_cert_chain("company.openapi.com")
</pre>
<pre>
Get-ChildItem | Sort-Object -Descending -Property @{Expression={$_.Length}} | Format-Table Name, @{Name="Size (MB)"; Expression={"{0:N2}" -f ($_.Length/1MB)}}
</pre>
<pre>
from datetime import datetime
import calendar

date_str = "2024-12"

# Parse year and month
year, month = map(int, date_str.split("-"))

# Get the last day of the month
last_day = calendar.monthrange(year, month)[1]

# Build the final date
last_date = datetime(year, month, last_day)

# Format as dd/mm/yyyy
formatted = last_date.strftime("%d/%m/%Y")

print(formatted)  # 31/12/2024

</pre>
<pre>
import pandas as pd
import glob
import os

# folder path with your Excel files
folder_path = "path/to/your/folder"

# list all Excel files in the folder
files = glob.glob(os.path.join(folder_path, "*.xlsx"))

dfs = []

for file in files:
    try:
        df = pd.read_excel(file, sheet_name="ML_MESE")
        dfs.append(df)
    except ValueError:
        # this happens if the file doesn't have the sheet ML_MESE
        print(f"Sheet 'ML_MESE' not found in {file}")

# concatenate all into one DataFrame
final_df = pd.concat(dfs, ignore_index=True)

print(final_df.shape)

</pre>
<pre>
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, FloatType, 
    BooleanType, DateType, TimestampType, DecimalType
)

def bq_to_spark_type(bq_field):
    """
    Convert BigQuery field to PySpark data type.
    bq_field: google.cloud.bigquery.SchemaField object
    """
    bq_type = bq_field.field_type
    # For NUMERIC/BIGNUMERIC, use precision and scale if available
    if bq_type in ["NUMERIC", "BIGNUMERIC"]:
        precision = getattr(bq_field, "precision", 38)  # Default BigQuery NUMERIC precision
        scale = getattr(bq_field, "scale", 9)           # Default BigQuery NUMERIC scale
        return DecimalType(precision=precision, scale=scale)
    mapping = {
        "STRING": StringType(),
        "INT64": IntegerType(),
        "FLOAT64": FloatType(),
        "BOOLEAN": BooleanType(),
        "DATE": DateType(),
        "TIMESTAMP": TimestampType(),
        "DATETIME": TimestampType()
    }
    return mapping.get(bq_type, StringType())

from google.cloud import bigquery

bq_client = bigquery.Client()
table_id = "your_project.your_dataset.your_table"
table = bq_client.get_table(table_id)

spark_schema = StructType([StructField(f.name, bq_to_spark_type(f), True) for f in table.schema])

</pre>
<pre>
df_with_ts = df.withColumn("event_ts", F.current_timestamp())

df_with_ts.write \
    .format("bigquery") \
    .option("table", "project_id.dataset.table_name") \
    .option("partitionField", "event_ts") \
    .option("partitionType", "DAY") \
    .option("partitionRequireFilter", "true") \
    .mode("append") \
    .save()
</pre>
<pre>
import ast
s = "{'a': 1, 'b': '2', 'c': 3}"
d = ast.literal_eval(s)
print(d)

for key, value in d.items():
    globals()[key] = value
</pre>
<pre>
from google.cloud import bigquery

client = bigquery.Client()

target_columns = ["id", "name", "age", "country"]
tables = [
    "project.dataset.table_202501",
    "project.dataset.table_202502",
    "project.dataset.table_202503"
]

def get_table_columns(table_name: str):
    """Fetch schema of a BigQuery table."""
    table = client.get_table(table_name)
    return [field.name for field in table.schema]

queries = []

for table_name in tables:
    existing_cols = get_table_columns(table_name)
    select_parts = []
    for col in target_columns:
        if col in existing_cols:
            select_parts.append(col)
        else:
            select_parts.append(f"NULL AS {col}")
    query = f"SELECT {', '.join(select_parts)} FROM `{table_name}`"
    queries.append(query)

# Combine them into a UNION ALL query
final_query = "\nUNION ALL\n".join(queries)

print(final_query)

</pre>
<pre>
from google.cloud import bigquery

client = bigquery.Client()

# Your target columns
target_columns = ["id", "name", "age", "country"]
tables = [
    "project.dataset.table_202501",
    "project.dataset.table_202502",
    "project.dataset.table_202503"
]

# Step 1: Collect schemas
table_schemas = {}
for table_name in tables:
    table = client.get_table(table_name)
    table_schemas[table_name] = {f.name: f.field_type for f in table.schema}

# Step 2: Decide a canonical type per column
# (Prefer STRING since it can represent most other types safely)
canonical_types = {col: "STRING" for col in target_columns}

# Step 3: Build aligned SELECT statements
queries = []
for table_name in tables:
    schema = table_schemas[table_name]
    select_parts = []
    for col in target_columns:
        if col in schema:
            # CAST if needed
            select_parts.append(f"CAST({col} AS {canonical_types[col]}) AS {col}")
        else:
            # Missing column -> NULL with cast
            select_parts.append(f"CAST(NULL AS {canonical_types[col]}) AS {col}")
    queries.append(f"SELECT {', '.join(select_parts)} FROM `{table_name}`")

# Step 4: Combine into final query
final_query = "\nUNION ALL\n".join(queries)

print(final_query)

</pre>
<pre>
import codecs

def convert_to_ebcdic_fixed_block(input_file, output_file, record_length=2179, encoding='cp037'):
    """
    Convert a text file to EBCDIC fixed block format.
    
    Args:
        input_file (str): Path to the input text file.
        output_file (str): Path to the output EBCDIC file.
        record_length (int): Fixed record length in bytes (default 2179).
        encoding (str): EBCDIC encoding (default IBM cp037, used in US).
    """
    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'wb') as outfile:
        for line in infile:
            # Remove newline characters and encode in EBCDIC
            ebcdic_line = line.rstrip('\r\n').encode(encoding)
            
            # Truncate or pad to fixed length
            if len(ebcdic_line) > record_length:
                ebcdic_line = ebcdic_line[:record_length]
            else:
                ebcdic_line = ebcdic_line.ljust(record_length, b' ')
            
            outfile.write(ebcdic_line)

    print(f"✅ Conversion complete. Output file: {output_file}")

# Example usage:
convert_to_ebcdic_fixed_block("input.txt", "output.ebc")

</pre>
<pre>
def convert_to_ascii_with_lf(input_file, output_file):
    """
    Convert a text file to ASCII with LF line endings.
    
    Args:
        input_file (str): Path to the input file (any encoding).
        output_file (str): Path to the output ASCII file.
    """
    with open(input_file, 'r', encoding='utf-8', errors='ignore') as infile, \
         open(output_file, 'w', encoding='ascii', errors='ignore', newline='\n') as outfile:
        
        for line in infile:
            # Normalize all line endings and write with LF
            cleaned_line = line.rstrip('\r\n')
            outfile.write(cleaned_line + '\n')

    print(f"✅ Converted to ASCII with LF line endings: {output_file}")

# Example usage:
convert_to_ascii_with_lf("input.txt", "output_ascii_lf.txt")

</pre>
<pre>
import codecs

def convert_ebcdic_fb_to_ascii(input_file, output_file, record_length=2179, encoding='cp037'):
    """
    Convert an EBCDIC fixed block file to ASCII text.
    
    Args:
        input_file (str): Path to the EBCDIC file (fixed block).
        output_file (str): Path to the output ASCII text file.
        record_length (int): Length of each fixed record in bytes (default 2179).
        encoding (str): EBCDIC code page (default 'cp037' for US).
    """
    with open(input_file, 'rb') as infile, open(output_file, 'w', encoding='utf-8', newline='\n') as outfile:
        while True:
            # Read one fixed block (record)
            record = infile.read(record_length)
            if not record:
                break

            # Decode EBCDIC to ASCII (UTF-8 text)
            ascii_line = record.decode(encoding, errors='ignore').rstrip()

            # Write each record as a line with LF
            outfile.write(ascii_line + '\n')

    print(f"✅ Conversion complete. Output file: {output_file}")

# Example usage:
convert_ebcdic_fb_to_ascii("input.ebc", "output_ascii.txt")

</pre>
<pre>
file_path = 'your_ebcdic_file.dat'
ebcdic_encoding = 'cp280' 
decoded_records = []

# 1. Open in Binary Read Mode ('rb')
try:
    with open(file_path, 'rb') as file:
        # 2. Iterate over the raw bytes, separated by the EBCDIC New Line byte (\x15)
        # The file.read().split() method reads the entire file into memory first.
        # For very large files, consider a streaming approach or mmap.
        
        # Note: The split byte \x15 is the EBCDIC equivalent of ASCII \n
        for ebcdic_record in file.read().split(b'\x15'):
            # The split operation may leave an empty byte string at the end
            if ebcdic_record: 
                # 3. Decode the byte record using the EBCDIC encoding
                try:
                    utf8_string = ebcdic_record.decode(ebcdic_encoding).strip()
                    decoded_records.append(utf8_string)
                except UnicodeDecodeError as e:
                    print(f"Skipping record due to decoding error: {e}")
                    # You may want to log the ebcdic_record here for inspection
                    
    print(f"Successfully decoded {len(decoded_records)} records.")
    
    # Example: Print the first 5 decoded records
    for i, record in enumerate(decoded_records[:5]):
        print(f"Record {i+1}: {record}")

except LookupError:
    print(f"Error: The encoding '{ebcdic_encoding}' is unknown. Try 'ibm280' or install the 'ebcdic' package.")
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
</pre>
<pre>
df = df.withColumn(c, F.regexp_replace(F.col(c), r"[\r\n]+", " "))
</pre>
<pre>
pdf.to_csv("output_ebcdic_cp280.csv", index=False, encoding="cp280", sep=";")
</pre>
<pre>
with open("output_utf8.csv", "r", encoding="utf-8") as fin, \
     open("output_cp280.csv", "wb") as fout:
    for line in fin:
        fout.write(line.encode("cp280"))
</pre>
<pre>
# Method 1: Using codecs
import codecs

input_file = 'input.ebc'
output_file = 'output.txt'

# Open the EBCDIC file with cp500 encoding
with codecs.open(input_file, 'r', encoding='cp500') as f_in:
    content = f_in.read()

# Write the content in UTF-8
with open(output_file, 'w', encoding='utf-8') as f_out:
    f_out.write(content)


# Method 2: Using open with encoding (Python 3+)
input_file = 'input.ebc'
output_file = 'output.txt'

with open(input_file, 'r', encoding='cp500') as f_in:
    content = f_in.read()

with open(output_file, 'w', encoding='utf-8') as f_out:
    f_out.write(content)

</pre>
<pre>
# Calculate the rank of 'salary' within each 'department'
# This requires a column to define the sort order within the partition, which is often handled
# by ensuring the underlying BigQuery execution includes the necessary ORDER BY.

# A common pattern is to use transform or apply with the required aggregation:
df['dept_rank'] = df.groupby('department')['salary'].transform(
    lambda x: x.rank(method='dense') # Applies dense rank within each department group
)
</pre>
<pre>
from pyspark.sql import functions as F, Window

w_pos = Window.partitionBy("group").orderBy("value")
w_neg = Window.partitionBy("group").orderBy(F.desc("value"))

df_selected = (
    df
    .withColumn("pos_rn", F.row_number().over(w_pos))   # smallest positive
    .withColumn("neg_rn", F.row_number().over(w_neg))   # largest negative (closest to 0)
    .filter(
        ((F.col("value") > 0) & (F.col("pos_rn") == 1)) |
        ((F.col("value") < 0) & (F.col("neg_rn") == 1))
    )
    .drop("pos_rn", "neg_rn")
)

df_flagged = (
    df.withColumn(
        "no_negative_flag",
        F.when(F.min(F.col("value") < 0).over(w) == False, "Y").otherwise("N")
    )
)

</pre>
<pre>
from pyspark.sql import functions as F

def add_missing_cols(df, cols):
    missing = set(cols) - set(df.columns)
    for col in missing:
        df = df.withColumn(col, F.lit(None))
    return df.select(cols)  # reorder columns
Step 3 — Apply the function to both DataFrames
python
Copia codice
df1_fixed = add_missing_cols(df1, cols)
df2_fixed = add_missing_cols(df2, cols)

df_union = df1_fixed.union(df2_fixed)
</pre>
<pre>
from pyspark.sql import functions as F

# 1. Columns of df1 (keep this order)
cols_df1 = df1.columns

# 2. Columns of df2 not present in df1 (append these at the end)
cols_df2_extra = [c for c in df2.columns if c not in cols_df1]

# 3. Final ordered column list
final_cols = cols_df1 + cols_df2_extra

# ------------ helper to add missing columns and reorder -------------
def align_columns(df, final_cols):
    missing = set(final_cols) - set(df.columns)
    for col in missing:
        df = df.withColumn(col, F.lit(None))
    return df.select(final_cols)

# 4. Align both DataFrames
df1_aligned = align_columns(df1, final_cols)
df2_aligned = align_columns(df2, final_cols)

# 5. Union
df_union = df1_aligned.unionByName(df2_aligned)
</pre>