<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>python Tips</h1>
<h3>Config</h3>
<pre>
list1 = ['a', 'b', 'c', 'd']
list2 = ['b', 'x', 'a', 'z', 'c']

filtered_list2 = [item in list1]
filtered_list2 = [item for item in list2 if item in list1]

print(filtered_list2)

</pre>
<pre>
from google.cloud.logging_v2.services.logging_service_v2 import LoggingServiceV2Client

</pre>
<pre>
from datetime import datetime, timedelta

project_id = "your-gcp-project-id"
log_filter = 'resource.type="cloud_function" AND severity>=ERROR'

# Example: Last 24 hours
start = datetime.utcnow() - timedelta(days=1)
end = datetime.utcnow()

logs = read_logs(project_id, log_filter, start, end)

for entry in logs:
    print(f"{entry.timestamp} - {entry.severity} - {entry.text_payload or entry.json_payload}")

</pre>
<pre>
import holidays
from datetime import date, datetime

def is_working_day(input_date=None, country='US'):
    """
    Checks if a given date (or today if not provided) is a working day.
    Excludes weekends and public holidays.

    Args:
        input_date (str or datetime.date or datetime.datetime, optional): 
            The date to check. If None, uses today. 
            If string, format should be 'YYYY-MM-DD'.
        country (str): Country code for holidays (default 'US').

    Returns:
        bool: True if it's a working day, False otherwise.
    """
    if input_date is None:
        check_date = date.today()
    elif isinstance(input_date, str):
        check_date = datetime.strptime(input_date, "%Y-%m-%d").date()
    elif isinstance(input_date, (date, datetime)):
        check_date = input_date.date() if isinstance(input_date, datetime) else input_date
    else:
        raise ValueError("Invalid date format. Use None, a 'YYYY-MM-DD' string, or a date object.")
    
    holiday_dates = holidays.country_holidays(country)
    
    return check_date.weekday() < 5 and check_date not in holiday_dates

</pre>
<pre>
df_with_new_date = df.withColumn("new_date", add_months(df["start_date"], 3))
</pre>
<pre>
import sys
print(f"Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}")
print("PySpark version:", spark.version)
</pre>
<pre>
import pandas as pd
import io
import sys

# Create a sample DataFrame
data = {
    'col1': [1, 2, 3, 4, 5],
    'col2': ['A', 'B', 'C', 'D', 'E'],
    'col3': [10.1, 20.2, None, 40.4, 50.5],
    'col4': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'])
}
df = pd.DataFrame(data)

# Specify the filename for the output
output_filename = "df_info_output.txt"

# Create a string buffer to capture the output
buffer = io.StringIO()

# Redirect stdout to the buffer
sys.stdout = buffer

# Call df.info() - its output will now go to the buffer
df.info()

# Restore stdout to its original state (the console)
sys.stdout = sys.__stdout__

# Get the captured output from the buffer
captured_output = buffer.getvalue()

# Write the captured output to a text file
with open(output_filename, 'w') as f:
    f.write(captured_output)

print(f"df.info() output saved to '{output_filename}'")

# You can also print the captured output to verify
# print("\n--- Captured Output ---")
# print(captured_output)
</pre>
<pre>
from datetime import datetime

filename = f"file_strs_{datetime.now().strftime('%Y_%m')}.ok"
print(filename)
</pre>
<pre>
from google.cloud import bigquery

# Initialize BigQuery client
client = bigquery.Client()

# Define your table in the format: project.dataset.table
table_id = "your-project.your_dataset.your_table"

# Get the table metadata
table = client.get_table(table_id)

# Get the last modified time (returns timestamp in milliseconds)
last_modified_time = table.modified  # This is a datetime.datetime object

print(f"Last modified time: {last_modified_time}")
</pre>
<pre>
from google.cloud import storage
from datetime import datetime

def get_latest_file_from_gcs(bucket_name, folder='', prefix='', extension=''):
    """
    Returns the latest file in a GCS bucket directory based on last modified time.

    Parameters:
    - bucket_name (str): Name of the GCS bucket.
    - folder (str): Folder path inside the bucket ('' means root). Should not start with '/'.
    - prefix (str): Prefix of the file name.
    - extension (str): File extension to filter (e.g. '.csv', '.json').

    Returns:
    - str: Full GCS path of the latest file (gs://bucket_name/path/to/file).
    """
    client = storage.Client()
    bucket = client.bucket(bucket_name)

    # Normalize folder path
    folder_prefix = folder.strip('/')
    full_prefix = f"{folder_prefix}/{prefix}".lstrip('/') if folder_prefix else prefix

    blobs = client.list_blobs(bucket_name, prefix=full_prefix)

    # Filter by extension and track latest blob
    latest_blob = None
    for blob in blobs:
        if extension and not blob.name.endswith(extension):
            continue
        if latest_blob is None or blob.updated > latest_blob.updated:
            latest_blob = blob

    if latest_blob:
        return f"gs://{bucket_name}/{latest_blob.name}"
    else:
        return None

</pre>
<pre>
import re

text = """
Some SQL or text that mentions pwcc_dqm.tab1, pwcc_dqm.tab2, and maybe pwcc_dqm.tab3
even inside longer strings or comments like 'select * from pwcc_dqm.tab4'.
"""

# Regular expression to match pwcc_dqm.<table_name>
matches = re.findall(r'\bpwcc_dqm\.\w+\b', text)

</pre>
<pre>
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

# Initialize client
client = bigquery.Client()

# Define table ID: 'project.dataset.table'
table_id = "your-project.your_dataset.your_table"

try:
    client.get_table(table_id)  # API request
    print(f"✅ Table {table_id} exists.")
    table_exists = True
except NotFound:
    print(f"❌ Table {table_id} does not exist.")
    table_exists = False

bool_value = value.lower() == "true"
</pre>
<pre>
from datetime import datetime

# Get today's date
now = datetime.now()

# Format the date strings
date_str = now.strftime("%Y%m%d")
datetime_str = now.strftime("%Y%m%d%H%M%S")

# Build the filename
filename = f"125479-4_Log_auditing_{date_str}_{datetime_str}.csv"

print(filename)

</pre>
<pre>
import subprocess

result = subprocess.run(["ls", "-l"], capture_output=True, text=True)
print(result.stdout)

</pre>
<pre>
import requests
from requests.exceptions import RequestException
import os

def check_api_connection(api_url):
    """
    Checks if a public API endpoint is reachable using a HEAD request.

    :param api_url: The full URL of the public API endpoint to check.
    :return: True if the connection is successful, False otherwise.
    """
    # No headers are needed for public APIs that don't require authentication.
    print(f"Attempting to connect to: {api_url}")
    print("-" * 30)

    try:
        # Use requests.head() to send a HEAD request.
        # The timeout parameter prevents the script from hanging indefinitely.
        response = requests.head(api_url, timeout=5)

        # A status code in the 200 range (2xx) indicates success.
        if response.status_code >= 200 and response.status_code < 300:
            print("✅ Connection successful!")
            print(f"Status Code: {response.status_code}")
            return True
        else:
            print(f"❌ Connection failed. Status Code: {response.status_code}")
            # The 'reason' provides a human-readable explanation of the status code
            print(f"Reason: {response.reason}")
            return False

    except RequestException as e:
        # This catches any network-related errors (e.g., DNS failure, connection refused).
        print(f"❌ An error occurred: {e}")
        return False

# --- Main part of the script ---
if __name__ == "__main__":
    # This is a public API endpoint that doesn't require an API key.
    # It's a great choice for testing the script's functionality.
    PUBLIC_API_URL = "https://jsonplaceholder.typicode.com/posts/1"

    check_api_connection(PUBLIC_API_URL)
</pre>
<pre>
import requests
from requests.exceptions import RequestException
import os
import urllib3

# Disable SSL warnings for unverified HTTPS requests
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def check_api_connection(api_url):
    """
    Checks if a public API endpoint is reachable using a HEAD request.

    :param api_url: The full URL of the public API endpoint to check.
    :return: True if the connection is successful, False otherwise.
    """
    print(f"Attempting to connect to: {api_url}")
    print("-" * 30)

    try:
        # Add verify=False to skip SSL certificate verification
        response = requests.head(api_url, timeout=5, verify=False)

        if 200 <= response.status_code < 300:
            print("✅ Connection successful!")
            print(f"Status Code: {response.status_code}")
            return True
        else:
            print(f"❌ Connection failed. Status Code: {response.status_code}")
            print(f"Reason: {response.reason}")
            return False

    except RequestException as e:
        print(f"❌ An error occurred: {e}")
        return False

# --- Main part of the script ---
if __name__ == "__main__":
    PUBLIC_API_URL = "https://jsonplaceholder.typicode.com/posts/1"
    check_api_connection(PUBLIC_API_URL)
</pre>
<pre>
from datetime import date, timedelta
import calendar

# Get today's date
today = date.today()

# Get the first day of the current month
first_day_this_month = today.replace(day=1)

# Subtract one day → last day of previous month
last_day_prev_month = first_day_this_month - timedelta(days=1)

# Format as string
result = last_day_prev_month.strftime("%Y-%m-%d")
print(result)

</pre>
<pre>
from datetime import datetime

# Get the current date and time
current_date = datetime.now()

# Extract the month as a number
month_number = current_date.month

if month_number in [1,4,7,10]:
  print("trimestre")
else:
  print("mensile")
</pre>
<pre>
from datetime import datetime, timedelta

# Get the current date
current_date = datetime.now()

# Get the first day of the current month
first_day_of_current_month = current_date.replace(day=1)

# Subtract one day to get the last day of the previous month
last_day_of_previous_month = first_day_of_current_month - timedelta(days=1)

# Format the date as a string in 'YYYY-MM-DD' format
last_day_str = last_day_of_previous_month.strftime('%Y-%m-%d')

# Print the final result
print(f"The last day of the previous month is: {last_day_str}")
</pre>
<pre>
import requests
from requests.exceptions import RequestException
import certifi

def check_api_connection(api_url):
    """
    Checks if a public API endpoint is reachable using a HEAD request.

    :param api_url: The full URL of the public API endpoint to check.
    :return: True if the connection is successful, False otherwise.
    """
    print(f"Attempting to connect to: {api_url}")
    print(f"Using certifi CA bundle: {certifi.where()}")
    print("-" * 30)

    try:
        # Use certifi's CA bundle for SSL verification
        response = requests.head(api_url, timeout=5, verify=certifi.where())

        if 200 <= response.status_code < 300:
            print("✅ Connection successful!")
            print(f"Status Code: {response.status_code}")
            return True
        else:
            print(f"❌ Connection failed. Status Code: {response.status_code}")
            print(f"Reason: {response.reason}")
            return False

    except RequestException as e:
        print(f"❌ An error occurred: {e}")
        return False

# --- Main part of the script ---
if __name__ == "__main__":
    PUBLIC_API_URL = "https://jsonplaceholder.typicode.com/posts/1"
    check_api_connection(PUBLIC_API_URL)

</pre>
<pre>
import ssl
import socket

def fetch_cert_chain(hostname, port=443, output_file="company_ca_bundle.pem"):
    # Create a connection and wrap with SSL to fetch certs
    ctx = ssl.create_default_context()
    conn = ctx.wrap_socket(socket.socket(socket.AF_INET), server_hostname=hostname)
    conn.connect((hostname, port))

    # Get the full certificate chain
    der_certs = conn.getpeercert(True)
    chain = conn.getpeercertchain() if hasattr(conn, "getpeercertchain") else [der_certs]

    conn.close()

    # Write the certs to a PEM file
    with open(output_file, "wb") as f:
        for cert in chain:
            pem = ssl.DER_cert_to_PEM_cert(cert)
            f.write(pem.encode("utf-8"))

    print(f"✅ Certificate chain for {hostname} saved to {output_file}")


if __name__ == "__main__":
    fetch_cert_chain("company.openapi.com")
</pre>
<pre>
Get-ChildItem | Sort-Object -Descending -Property @{Expression={$_.Length}} | Format-Table Name, @{Name="Size (MB)"; Expression={"{0:N2}" -f ($_.Length/1MB)}}
</pre>
<pre>
from datetime import datetime
import calendar

date_str = "2024-12"

# Parse year and month
year, month = map(int, date_str.split("-"))

# Get the last day of the month
last_day = calendar.monthrange(year, month)[1]

# Build the final date
last_date = datetime(year, month, last_day)

# Format as dd/mm/yyyy
formatted = last_date.strftime("%d/%m/%Y")

print(formatted)  # 31/12/2024

</pre>
<pre>
import pandas as pd
import glob
import os

# folder path with your Excel files
folder_path = "path/to/your/folder"

# list all Excel files in the folder
files = glob.glob(os.path.join(folder_path, "*.xlsx"))

dfs = []

for file in files:
    try:
        df = pd.read_excel(file, sheet_name="ML_MESE")
        dfs.append(df)
    except ValueError:
        # this happens if the file doesn't have the sheet ML_MESE
        print(f"Sheet 'ML_MESE' not found in {file}")

# concatenate all into one DataFrame
final_df = pd.concat(dfs, ignore_index=True)

print(final_df.shape)

</pre>
<pre>
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, FloatType, 
    BooleanType, DateType, TimestampType, DecimalType
)

def bq_to_spark_type(bq_field):
    """
    Convert BigQuery field to PySpark data type.
    bq_field: google.cloud.bigquery.SchemaField object
    """
    bq_type = bq_field.field_type
    # For NUMERIC/BIGNUMERIC, use precision and scale if available
    if bq_type in ["NUMERIC", "BIGNUMERIC"]:
        precision = getattr(bq_field, "precision", 38)  # Default BigQuery NUMERIC precision
        scale = getattr(bq_field, "scale", 9)           # Default BigQuery NUMERIC scale
        return DecimalType(precision=precision, scale=scale)
    mapping = {
        "STRING": StringType(),
        "INT64": IntegerType(),
        "FLOAT64": FloatType(),
        "BOOLEAN": BooleanType(),
        "DATE": DateType(),
        "TIMESTAMP": TimestampType(),
        "DATETIME": TimestampType()
    }
    return mapping.get(bq_type, StringType())

from google.cloud import bigquery

bq_client = bigquery.Client()
table_id = "your_project.your_dataset.your_table"
table = bq_client.get_table(table_id)

spark_schema = StructType([StructField(f.name, bq_to_spark_type(f), True) for f in table.schema])

</pre>
<pre>
df_with_ts = df.withColumn("event_ts", F.current_timestamp())

df_with_ts.write \
    .format("bigquery") \
    .option("table", "project_id.dataset.table_name") \
    .option("partitionField", "event_ts") \
    .option("partitionType", "DAY") \
    .option("partitionRequireFilter", "true") \
    .mode("append") \
    .save()
</pre>
<pre>
import ast
s = "{'a': 1, 'b': '2', 'c': 3}"
d = ast.literal_eval(s)
print(d)

for key, value in d.items():
    globals()[key] = value
</pre>
<pre>
from google.cloud import bigquery

client = bigquery.Client()

target_columns = ["id", "name", "age", "country"]
tables = [
    "project.dataset.table_202501",
    "project.dataset.table_202502",
    "project.dataset.table_202503"
]

def get_table_columns(table_name: str):
    """Fetch schema of a BigQuery table."""
    table = client.get_table(table_name)
    return [field.name for field in table.schema]

queries = []

for table_name in tables:
    existing_cols = get_table_columns(table_name)
    select_parts = []
    for col in target_columns:
        if col in existing_cols:
            select_parts.append(col)
        else:
            select_parts.append(f"NULL AS {col}")
    query = f"SELECT {', '.join(select_parts)} FROM `{table_name}`"
    queries.append(query)

# Combine them into a UNION ALL query
final_query = "\nUNION ALL\n".join(queries)

print(final_query)

</pre>
<pre>
from google.cloud import bigquery

client = bigquery.Client()

# Your target columns
target_columns = ["id", "name", "age", "country"]
tables = [
    "project.dataset.table_202501",
    "project.dataset.table_202502",
    "project.dataset.table_202503"
]

# Step 1: Collect schemas
table_schemas = {}
for table_name in tables:
    table = client.get_table(table_name)
    table_schemas[table_name] = {f.name: f.field_type for f in table.schema}

# Step 2: Decide a canonical type per column
# (Prefer STRING since it can represent most other types safely)
canonical_types = {col: "STRING" for col in target_columns}

# Step 3: Build aligned SELECT statements
queries = []
for table_name in tables:
    schema = table_schemas[table_name]
    select_parts = []
    for col in target_columns:
        if col in schema:
            # CAST if needed
            select_parts.append(f"CAST({col} AS {canonical_types[col]}) AS {col}")
        else:
            # Missing column -> NULL with cast
            select_parts.append(f"CAST(NULL AS {canonical_types[col]}) AS {col}")
    queries.append(f"SELECT {', '.join(select_parts)} FROM `{table_name}`")

# Step 4: Combine into final query
final_query = "\nUNION ALL\n".join(queries)

print(final_query)

</pre>
<pre>
import codecs

def convert_to_ebcdic_fixed_block(input_file, output_file, record_length=2179, encoding='cp037'):
    """
    Convert a text file to EBCDIC fixed block format.
    
    Args:
        input_file (str): Path to the input text file.
        output_file (str): Path to the output EBCDIC file.
        record_length (int): Fixed record length in bytes (default 2179).
        encoding (str): EBCDIC encoding (default IBM cp037, used in US).
    """
    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'wb') as outfile:
        for line in infile:
            # Remove newline characters and encode in EBCDIC
            ebcdic_line = line.rstrip('\r\n').encode(encoding)
            
            # Truncate or pad to fixed length
            if len(ebcdic_line) > record_length:
                ebcdic_line = ebcdic_line[:record_length]
            else:
                ebcdic_line = ebcdic_line.ljust(record_length, b' ')
            
            outfile.write(ebcdic_line)

    print(f"✅ Conversion complete. Output file: {output_file}")

# Example usage:
convert_to_ebcdic_fixed_block("input.txt", "output.ebc")

</pre>