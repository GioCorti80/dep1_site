<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>bq Tips</h1>
<h3>Config</h3>
<pre>
    SELECT 
    CAST(123.456 AS NUMERIC) AS casted_value, -- Results in 123.456 (NUMERIC has up to 9 decimal places)
    CAST(ROUND(123.456, 2) AS NUMERIC) AS rounded_value -- Results in 123.46
</pre>
<pre>
    df.to_gbq(table_name, if_exists='replace')
    
    SELECT birth_date, DATE_ADD(birth_date, INTERVAL 90 YEAR) AS date_at_90
</pre>
<pre>
SELECT 
    table_name,
    column_name,
    data_type
FROM 
    `project_id.dataset_id.INFORMATION_SCHEMA.COLUMNS`
WHERE 
    table_name = 'your_table_name';
</pre>
<h4>STORED PROCEDURES</h4>
<pre>
CREATE OR REPLACE PROCEDURE my_dataset.get_user_details(IN user_id INT64)
BEGIN
    SELECT * FROM my_dataset.users WHERE id = user_id;
END;
</pre>
<pre>schedule="0 19 * * 3",  # Runs every Wednesday at 19:00 UTC</pre>

<pre>PARSE DATE
    SELECT PARSE_DATE('%Y-%m-%d', date_string) AS parsed_date
    FROM your_table;
   
</pre>

<pre>
    df = spark.read.format("bigquery") \
    .option("table", "your_project.your_dataset.your_table") \
    .load()
</pre>
<pre>
from pyspark.sql import SparkSession
from google.cloud import bigquery
from pyspark.sql.types import StructType, StructField, StringType

# Define table parameters
project_id = "your_project_id"
dataset = "your_dataset"
table = "your_table"
table_id = f"{project_id}.{dataset}.{table}"

# Initialize BigQuery client
bq_client = bigquery.Client()

# Check if table exists in BigQuery
def table_exists_bq(table_id):
    try:
        bq_client.get_table(table_id)
        return True
    except Exception:
        return False

# Main logic
if table_exists_bq(table_id):
    df = spark.read.format("bigquery").option("table", table_id).load()
else:
    schema = StructType([
        StructField("ndg", StringType(), True),
        StructField("codice_evento", StringType(), True)
    ])
    df = spark.createDataFrame([], schema)

# Now `df` is either read from BigQuery or an empty DataFrame with the required schema
df.show()
    
</pre>
<pre>
LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))
DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH)
</pre>
<pre>
SELECT DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)
</pre>
<pre>DATE_TRUNC(DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH), QUARTER) and DATE_TRUNC(CURRENT_DATE, QUARTER) - 1</pre>
<pre>
^\[process \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}:\d{2}\] -\s*
^\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}:\d+\+\d{2}:\d{2}\] \{.*?\} INFO -\s*
\b(pwcc_dataset\.\w+)\b
'db-project.\1'


pd.to_datetime(df['date_str'], format='%d/%m/%Y')
</pre>
<pre>
def append_dataframe_to_bigquery_table(dataframe, table_id, partition_field, is_partitioned=True, write_mode="append", replace_partition=False):
    """
    Appends a PySpark DataFrame to an existing BigQuery table, with options for partitioning, write mode, and replacing a single partition.

    Args:
        dataframe (DataFrame): The PySpark DataFrame to append.
        table_id (str): The ID of the BigQuery table to append to.
        partition_field (str): The name of the date column to partition by.
        is_partitioned (bool): Whether the table is partitioned. Defaults to True.
        write_mode (str): The write mode ("append" or "overwrite"). Defaults to "append".
        replace_partition (bool): Whether to replace a single partition. Defaults to False.
    """
    if replace_partition and write_mode.lower() == "overwrite":
        raise ValueError("Cannot use replace_partition with overwrite write mode.")

    if replace_partition:
        # Delete the existing partition
        partition_value = dataframe.select(partition_field).first()[0]
        spark.read.format("bigquery") \
            .option("table", table_id) \
            .option("partitionField", partition_field) \
            .option("partitionValue", partition_value) \
            .load() \
            .createOrReplaceTempView("partition_to_delete")
        spark.sql(f"DELETE FROM `{table_id}` WHERE `{partition_field}` = '{partition_value}'")

    dataframe.write \
        .format("bigquery") \
        .option("table", table_id) \
        .option("partitionField", partition_field) \
        .option("writeDisposition", write_mode.upper()) \
        .option("createDisposition", "CREATE_IF_NEEDED") \
        .option("requirePartitionFilter", is_partitioned) \
        .save()
</pre>