<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>bq Tips</h1>
<h3>Config</h3>
<pre>
    SELECT 
    CAST(123.456 AS NUMERIC) AS casted_value, -- Results in 123.456 (NUMERIC has up to 9 decimal places)
    CAST(ROUND(123.456, 2) AS NUMERIC) AS rounded_value -- Results in 123.46
</pre>
<pre>
    df.to_gbq(table_name, if_exists='replace')
    
    SELECT birth_date, DATE_ADD(birth_date, INTERVAL 90 YEAR) AS date_at_90
</pre>
<pre>
SELECT 
    table_name,
    column_name,
    data_type
FROM 
    `project_id.dataset_id.INFORMATION_SCHEMA.COLUMNS`
WHERE 
    table_name = 'your_table_name';
</pre>
<h4>STORED PROCEDURES</h4>
<pre>
CREATE OR REPLACE PROCEDURE my_dataset.get_user_details(IN user_id INT64)
BEGIN
    SELECT * FROM my_dataset.users WHERE id = user_id;
END;
</pre>
<pre>schedule="0 19 * * 3",  # Runs every Wednesday at 19:00 UTC</pre>

<pre>PARSE DATE
    SELECT PARSE_DATE('%Y-%m-%d', date_string) AS parsed_date
    FROM your_table;
   
</pre>

<pre>
    df = spark.read.format("bigquery") \
    .option("table", "your_project.your_dataset.your_table") \
    .load()
</pre>
<pre>
from pyspark.sql import SparkSession
from google.cloud import bigquery
from pyspark.sql.types import StructType, StructField, StringType

# Define table parameters
project_id = "your_project_id"
dataset = "your_dataset"
table = "your_table"
table_id = f"{project_id}.{dataset}.{table}"

# Initialize BigQuery client
bq_client = bigquery.Client()

# Check if table exists in BigQuery
def table_exists_bq(table_id):
    try:
        bq_client.get_table(table_id)
        return True
    except Exception:
        return False

# Main logic
if table_exists_bq(table_id):
    df = spark.read.format("bigquery").option("table", table_id).load()
else:
    schema = StructType([
        StructField("ndg", StringType(), True),
        StructField("codice_evento", StringType(), True)
    ])
    df = spark.createDataFrame([], schema)

# Now `df` is either read from BigQuery or an empty DataFrame with the required schema
df.show()
    
</pre>
<pre>
LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))
DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH)
</pre>
<pre>
SELECT DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)
</pre>
<pre>DATE_TRUNC(DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH), QUARTER) and DATE_TRUNC(CURRENT_DATE, QUARTER) - 1</pre>
<pre>
^\[process \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}:\d{2}\] -\s*
^\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}:\d+\+\d{2}:\d{2}\] \{.*?\} INFO -\s*
\b(pwcc_dataset\.\w+)\b
'db-project.\1'


pd.to_datetime(df['date_str'], format='%d/%m/%Y')
</pre>