<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>bq Tips</h1>
<h3>Config</h3>
<pre>
    SELECT 
    CAST(123.456 AS NUMERIC) AS casted_value, -- Results in 123.456 (NUMERIC has up to 9 decimal places)
    CAST(ROUND(123.456, 2) AS NUMERIC) AS rounded_value -- Results in 123.46
</pre>
<pre>
    df.to_gbq(table_name, if_exists='replace')
    
    SELECT birth_date, DATE_ADD(birth_date, INTERVAL 90 YEAR) AS date_at_90
</pre>
<pre>
SELECT 
    table_name,
    column_name,
    data_type
FROM 
    `project_id.dataset_id.INFORMATION_SCHEMA.COLUMNS`
WHERE 
    table_name = 'your_table_name';
</pre>
<h4>STORED PROCEDURES</h4>
<pre>
CREATE OR REPLACE PROCEDURE my_dataset.get_user_details(IN user_id INT64)
BEGIN
    SELECT * FROM my_dataset.users WHERE id = user_id;
END;
</pre>
<pre>schedule="0 19 * * 3",  # Runs every Wednesday at 19:00 UTC</pre>

<pre>PARSE DATE
    SELECT PARSE_DATE('%Y-%m-%d', date_string) AS parsed_date
    FROM your_table;
   
</pre>

<pre>
    df = spark.read.format("bigquery") \
    .option("table", "your_project.your_dataset.your_table") \
    .load()
</pre>
<pre>
from pyspark.sql import SparkSession
from google.cloud import bigquery
from pyspark.sql.types import StructType, StructField, StringType

# Define table parameters
project_id = "your_project_id"
dataset = "your_dataset"
table = "your_table"
table_id = f"{project_id}.{dataset}.{table}"

# Initialize BigQuery client
bq_client = bigquery.Client()

# Check if table exists in BigQuery
def table_exists_bq(table_id):
    try:
        bq_client.get_table(table_id)
        return True
    except Exception:
        return False

# Main logic
if table_exists_bq(table_id):
    df = spark.read.format("bigquery").option("table", table_id).load()
else:
    schema = StructType([
        StructField("ndg", StringType(), True),
        StructField("codice_evento", StringType(), True)
    ])
    df = spark.createDataFrame([], schema)

# Now `df` is either read from BigQuery or an empty DataFrame with the required schema
df.show()
    
</pre>
<pre>
LAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH))
DATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 2 MONTH), MONTH)
</pre>
<pre>
SELECT DATE_SUB(DATE_TRUNC(CURRENT_DATE(), MONTH), INTERVAL 1 DAY)
</pre>
<pre>DATE_TRUNC(DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH), QUARTER) and DATE_TRUNC(CURRENT_DATE, QUARTER) - 1</pre>
<pre>
^\[process \d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}:\d{2}\] -\s*
^\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}:\d+\+\d{2}:\d{2}\] \{.*?\} INFO -\s*
\b(pwcc_dataset\.\w+)\b
'db-project.\1'


pd.to_datetime(df['date_str'], format='%d/%m/%Y')
</pre>
<pre>
def append_dataframe_to_bigquery_table(dataframe, table_id, partition_field, is_partitioned=True, write_mode="append", replace_partition=False):
    """
    Appends a PySpark DataFrame to an existing BigQuery table, with options for partitioning, write mode, and replacing a single partition.

    Args:
        dataframe (DataFrame): The PySpark DataFrame to append.
        table_id (str): The ID of the BigQuery table to append to.
        partition_field (str): The name of the date column to partition by.
        is_partitioned (bool): Whether the table is partitioned. Defaults to True.
        write_mode (str): The write mode ("append" or "overwrite"). Defaults to "append".
        replace_partition (bool): Whether to replace a single partition. Defaults to False.
    """
    if replace_partition and write_mode.lower() == "overwrite":
        raise ValueError("Cannot use replace_partition with overwrite write mode.")

    if replace_partition:
        # Delete the existing partition
        partition_value = dataframe.select(partition_field).first()[0]
        spark.read.format("bigquery") \
            .option("table", table_id) \
            .option("partitionField", partition_field) \
            .option("partitionValue", partition_value) \
            .load() \
            .createOrReplaceTempView("partition_to_delete")
        spark.sql(f"DELETE FROM `{table_id}` WHERE `{partition_field}` = '{partition_value}'")

    dataframe.write \
        .format("bigquery") \
        .option("table", table_id) \
        .option("partitionField", partition_field) \
        .option("writeDisposition", write_mode.upper()) \
        .option("createDisposition", "CREATE_IF_NEEDED") \
        .option("requirePartitionFilter", is_partitioned) \
        .save()
</pre>
<pre>
SELECT
  *,
  MAX(other_date_column) OVER (
    PARTITION BY group_column
    ORDER BY UNIX_MICROS(timestamp_column)
    RANGE BETWEEN CURRENT ROW AND 47336400000000 FOLLOWING
  ) AS max_other_date_within_18_months
FROM
  your_table;

SELECT
  *,
  MAX(other_date_column) OVER (
    PARTITION BY group_column
    ORDER BY UNIX_MICROS(timestamp_column)
    RANGE BETWEEN CURRENT ROW AND 31557600000000 FOLLOWING
  ) AS max_other_date_within_12_months
FROM
  your_table;
</pre>
<pre>
query = f"""
    SELECT
        partition_id,
        TIMESTAMP_MICROS(last_modified_time) AS last_updated_datetime
    FROM
        `{project_id}.{dataset_id}.INFORMATION_SCHEMA.PARTITIONS`
    WHERE
        table_name = '{table_name}'
    ORDER BY
        partition_id DESC
"""

# Execute the query
query_job = client.query(query)

# Fetch the results
results = query_job.result()
</pre>
<pre>
resource.type="bigquery_resource"
severity>=WARNING
text:("Read" OR "read session" OR "storage" OR "quota" OR "stream")
</pre>
<pre>
from google.cloud import bigquery

client = bigquery.Client()

source_table = "source-project.source_dataset.my_table"  
destination_table = "target-project.target_dataset.my_table"

job = client.copy_table(
    sources=source_table,
    destination=destination_table,
    job_config=bigquery.CopyJobConfig(write_disposition="WRITE_EMPTY")
)

job.result()  # Wait for completion
print("Table copied successfully!")
</pre>
<pre>
WHERE date_field BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL (EXTRACT(DAYOFWEEK FROM CURRENT_DATE()) + 5) DAY) AND DATE_SUB(CURRENT_DATE(), INTERVAL (EXTRACT(DAYOFWEEK FROM CURRENT_DATE()) - 2) DAY)

WITH date_ranges AS (
  SELECT
    -- Current date
    CURRENT_DATE() AS today,

    -- Day of the week (1 = Sunday, 7 = Saturday)
    EXTRACT(DAYOFWEEK FROM CURRENT_DATE()) AS day_of_week,

    -- Calculate days to subtract to get to the previous Monday
    -- If today is Tuesday (day 3), we subtract 6 days to get to the previous Monday
    -- This formula ensures we always get the previous Monday
    DATE_SUB(CURRENT_DATE(), INTERVAL ((EXTRACT(DAYOFWEEK FROM CURRENT_DATE()) + 5) % 7) DAY) AS previous_monday,

    -- Previous Friday is 4 days after the previous Monday
    DATE_ADD(DATE_SUB(CURRENT_DATE(), INTERVAL ((EXTRACT(DAYOFWEEK FROM CURRENT_DATE()) + 5) % 7) DAY, INTERVAL 4 DAY) AS previous_friday
)

SELECT
  *
FROM
  your_table
WHERE
  date_field BETWEEN (SELECT previous_monday FROM date_ranges) AND (SELECT previous_friday FROM date_ranges);

</pre>