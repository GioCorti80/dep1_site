<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>Bigframes</h1>
<h3>Config</h3>

<pre>
In Google BigFrames (Pandas API for BigQuery),
you can union multiple DataFrames with different columns using the union() method while aligning columns properly.
</pre>

<pre>
import bigframes.pandas as bpd

import bigframes.pandas as bpd

def convert_columns_to_string(df: bpd.DataFrame, columns: list) -> bpd.DataFrame:
    """
    Converts specified columns in a BigFrames DataFrame to string type.
    
    :param df: BigFrames DataFrame
    :param columns: List of column names to convert to string
    :return: BigFrames DataFrame with specified columns as strings
    """
    df[columns] = df[columns].astype(str)
    return df

# Example usage:
# df = bpd.read_gbq("SELECT * FROM my_dataset.my_table")
# df = convert_columns_to_string(df, ['column1', 'column2'])
# print(df.dtypes)
</pre>
<pre>
union between multiple dataframes with different columns
def union_dataframes(dfs):
  """
  Performs a union of a list of BigFrames DataFrames, handling different columns.

  Args:
    dfs: A list of BigFrames DataFrames.

  Returns:
    A single BigFrames DataFrame containing the union of all input DataFrames.
  """

  if not dfs:
    return bpd.DataFrame()  # Return an empty DataFrame if the input list is empty

  if len(dfs) == 1:
    return dfs[0]

  all_columns = set()
  for df in dfs:
    all_columns.update(df.columns)

  all_columns = list(all_columns)

  extended_dfs = []
  for df in dfs:
    missing_columns = [col for col in all_columns if col not in df.columns]
    extended_df = df.copy() # important to copy the dataframe to avoid altering the original
    for col in missing_columns:
      extended_df[col] = bpd.NA  # Add missing columns with NA values
    extended_dfs.append(extended_df[all_columns])

  result = bpd.concat(extended_dfs, ignore_index=True)
  return result
</pre>

<pre>
    def union_dataframes_with_type_handling(dfs):
    """
    Performs a union of a list of BigFrames DataFrames, handling different columns and data types.
  
    Args:
      dfs: A list of BigFrames DataFrames.
  
    Returns:
      A single BigFrames DataFrame containing the union of all input DataFrames.
    """
  
    if not dfs:
      return bpd.DataFrame()
  
    if len(dfs) == 1:
      return dfs[0]
  
    all_columns = set()
    column_types = {}  # Store column names and their inferred types
    for df in dfs:
      all_columns.update(df.columns)
      for col in df.columns:
        if col not in column_types:
          column_types[col] = df[col].dtype
        else:
          # If column exists in multiple dfs, choose the most general type to avoid errors.
          # This part is crucial for type compatibility.
          if (pd.api.types.is_numeric_dtype(df[col].dtype) and
              pd.api.types.is_numeric_dtype(column_types[col])):
            #If both are numeric, ensure same numeric type. if not, use float64
            if column_types[col]!=df[col].dtype:
              column_types[col] = 'float64'
          elif (pd.api.types.is_datetime64_any_dtype(df[col].dtype) and
                pd.api.types.is_datetime64_any_dtype(column_types[col])):
            column_types[col] = 'datetime64[ns]'
          elif pd.api.types.is_string_dtype(df[col].dtype) or pd.api.types.is_string_dtype(column_types[col]):
            column_types[col] = 'object'
          else:
            column_types[col] = 'object' #default to object if types are not compatible.
  
    all_columns = list(all_columns)
  
    extended_dfs = []
    for df in dfs:
      missing_columns = [col for col in all_columns if col not in df.columns]
      extended_df = df.copy()
      for col in missing_columns:
        extended_df[col] = bpd.NA
        extended_df = extended_df.astype({col: column_types[col]}) #cast the NA columns to the correct type
  
      for col in df.columns:
          extended_df = extended_df.astype({col:column_types[col]}) #cast existing columns to the correct type.
      extended_dfs.append(extended_df[all_columns])
  
    result = bpd.concat(extended_dfs, ignore_index=True)
    return result
</pre>
<pre>
import bigframes.pandas as bpd

# Assuming you have a BigFrames DataFrame named 'bf_df'
# Drop a single column
bf_df = bf_df.drop(columns=['column_to_drop'])

# Drop multiple columns
columns_to_drop = ['column_one', 'column_two', 'column_three']
bf_df = bf_df.drop(columns=columns_to_drop)

# To drop columns in place (modify the original DataFrame):
bf_df.drop(columns=columns_to_drop, inplace=True)
</pre>
<pre>
import bigframes.pandas as bpd

# Assuming you have a BigFrames DataFrame named 'bf_df'
# Rename a single column
bf_df = bf_df.rename(columns={'old_name': 'new_name'})

# Rename multiple columns
new_column_names = {'old_name_1': 'new_name_1',
                    'old_name_2': 'new_name_2',
                    'old_name_3': 'new_name_3'}
bf_df = bf_df.rename(columns=new_column_names)

# To rename columns in place:
bf_df.rename(columns=new_column_names, inplace=True)
</pre>
<pre>
substring:
bf_df['first_three'] = bf_df['original_string'].str[:3]
</pre>
<pre>
import bigframes.pandas as bpd
import numpy as np

# Load the table
df = bpd.read_gbq("SELECT name, score FROM your_dataset.students")

# Define conditions
conditions = [
    df["score"] >= 90,
    df["score"] >= 75
]
choices = ["Excellent", "Good"]

# Apply CASE WHEN using np.select
df["rating"] = np.select(conditions, choices, default="Needs Improvement")

# Optional: convert to Pandas to see the result locally
df.to_pandas()

</pre>