<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>cloud logging</h1>
<h3>Config</h3>
<pre>
resource.type="bigquery_project"
protoPayload.methodName="jobservice.jobcompleted"
protoPayload.serviceData.jobCompletedEvent.eventName="QUERY_JOB_COMPLETED"
</pre>
<pre>
from google.cloud import logging

client = logging.Client()
logger = client.logger('')

# Example advanced filter (same as UI)
filter_str = """
resource.type="bigquery_project"
protoPayload.methodName="jobservice.jobcompleted"
protoPayload.serviceData.jobCompletedEvent.eventName="QUERY_JOB_COMPLETED"
"""

entries = client.list_entries(filter_=filter_str)

for entry in entries:
    user_email = entry.payload['authenticationInfo']['principalEmail']
    print(f"User: {user_email} — Log time: {entry.timestamp}")
</pre>
<pre>
from google.cloud import logging

client = logging.Client()

# Same filter as before
filter_str = """
resource.type="bigquery_project"
protoPayload.methodName="jobservice.jobcompleted"
protoPayload.serviceData.jobCompletedEvent.eventName="QUERY_JOB_COMPLETED"
"""

entries = client.list_entries(filter_=filter_str)

for entry in entries:
    proto_payload = entry.proto_payload  # <-- Correct
    user_email = proto_payload.get('authenticationInfo', {}).get('principalEmail')
    print(f"User: {user_email} — Log time: {entry.timestamp}")
    
</pre>
<pre>
from pyspark.sql.functions import regexp_extract_all, col, regexp_replace, udf
from pyspark.sql.types import ArrayType, StringType

import re

# Define a UDF to extract all 12-digit numbers following 'ndg' in the query
def extract_ndg_ids(query):
    if query is None:
        return []
    # Make query lowercase for easier matching
    query = query.lower()
    
    # This regex finds 12-digit strings following 'ndg' patterns
    # Matches 'ndg = '...' or 'ndg in (...)'
    matches = re.findall(r"ndg\s*(?:=|in)\s*\(?\s*['\"](\d{12})['\"](?:\s*,\s*['\"](\d{12})['\"])*", query)

    ndg_ids = []

    for match in matches:
        for val in match:
            if val:  # avoid empty strings from optional groups
                ndg_ids.append(val)
    
    # Also find other 12-digit numbers in the context of ndg=...
    extra_matches = re.findall(r"ndg\s*=\s*['\"](\d{12})['\"]", query)
    ndg_ids.extend(extra_matches)

    return list(set(ndg_ids))  # deduplicate

# Register UDF
extract_ndg_ids_udf = udf(extract_ndg_ids, ArrayType(StringType()))

# Apply to DataFrame
df_with_ndg = df.withColumn("num", extract_ndg_ids_udf(col("query")))

</pre>
<pre>
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, regexp_extract, regexp_replace, split, explode

# Initialize Spark Session (if not already initialized)
spark = SparkSession.builder.appName("ExtractNDG").getOrCreate()

# Sample Data (replace with your actual DataFrame)
data = [
    ("select * from table where ndg in ('012345678912', '987654321012')",),
    ("select * from table where ndg='998877665544'",),
    ("select * from another_table where user_id = 'abc' and ndg in ('112233445566', '223344556677', '334455667788')",),
    ("select * from yet_another_table where some_column = 'value'",), # No NDG
    ("select * from sales where ndg = '123456789012' OR customer_type = 'premium'",)
]
df = spark.createDataFrame(data, ["query_sql"])

# Define the regular expression pattern
# This pattern looks for 'ndg=' or 'ndg in (' followed by one or more
# 12-digit numbers enclosed in single quotes, potentially separated by commas.
# It captures the numbers themselves.
ndg_pattern = r"ndg\s*(?:=|in\s*\()\s*('(\d{12})'(?:,\s*'(\d{12})')*)\)?"

# Extract the potential NDG string
# The first group (group 1) will capture '012345678912', '987654321012' or '012345678912'
df_extracted = df.withColumn(
    "extracted_ndg_string",
    regexp_extract(col("query_sql"), ndg_pattern, 1)
)

# Clean up the extracted string: remove quotes and spaces, then split by comma
df_cleaned = df_extracted.withColumn(
    "ndg_array_raw",
    split(regexp_replace(col("extracted_ndg_string"), r"['\s]", ""), ",")
)

# Filter out empty arrays that might result from no match or malformed matches
df_filtered_ndg = df_cleaned.withColumn(
    "ndg_array",
    when(size(col("ndg_array_raw")) > 0, col("ndg_array_raw")).otherwise(array())
)

# Explode the array to get a new row for each NDG
# This is useful if you want each NDG to be a separate entry.
df_exploded = df_filtered_ndg.withColumn("customer_id", explode(col("ndg_array")))

# Select and show the relevant columns
df_final = df_exploded.select("query_sql", "customer_id").filter(col("customer_id") != "")

df_final.show(truncate=False)

# If you prefer to keep all NDGs in a single column as an array or a comma-separated string,
# you can skip the explode step and simply cast or join the array:
df_array_ndg = df_filtered_ndg.select("query_sql", col("ndg_array").alias("customer_ids_array"))
df_array_ndg.show(truncate=False)

# If you want a comma-separated string:
from pyspark.sql.functions import array_join
df_string_ndg = df_filtered_ndg.withColumn("customer_ids_string", array_join(col("ndg_array"), ", "))
df_string_ndg.select("query_sql", "customer_ids_string").show(truncate=False)
</pre>