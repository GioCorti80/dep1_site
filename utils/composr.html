<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>Composer</h1>
<h3>Config</h3>
<pre>
If you want your DAG to run every Tuesday at 10:00 AM, update the schedule_interval to:
0 10 * * 2
</pre>

<pre>
    from datetime import datetime, timedelta
    
    def is_third_friday(execution_date: datetime) -> bool:
        """Return True only if the date is the 3rd Friday of the month"""
        if execution_date.weekday() != 4:  # 4 is Friday
            return False
        day = execution_date.day
        return 15 <= day <= 21
</pre>
<pre>
    0 10 * * 5
</pre>
<pre>
schedule_interval='0 11 7 * *'
</pre>
<pre>
0 10 1-5 * *
</pre>
<pre>
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from datetime import datetime
from google.cloud import bigquery

# Set your values
PROJECT_ID = "your-project"
DATASET = "your_dataset"
MAIN_TABLE = f"{PROJECT_ID}.{DATASET}.main_table"
STAGING_TABLE = f"{PROJECT_ID}.{DATASET}.staging_table"
CSV_PATH = "/path/to/update_file.csv"  # Update this

default_args = {
    "start_date": datetime(2024, 1, 1),
    "retries": 1,
}

with DAG("update_user_email_dag", schedule_interval=None, default_args=default_args, catchup=False) as dag:

def load_csv_to_bigquery():
    from google.cloud import bigquery

    client = bigquery.Client()

    # Define explicit schema
    schema = [
        bigquery.SchemaField("user_id", "STRING"),
        bigquery.SchemaField("user_email", "STRING"),
    ]

    job_config = bigquery.LoadJobConfig(
        schema=schema,
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,  # Skip headers even if wrong or messy
        write_disposition="WRITE_TRUNCATE",
        autodetect=False,  # Important: you want your schema, not inferred
    )

    with open("/path/to/update_file.csv", "rb") as source_file:
        load_job = client.load_table_from_file(
            source_file,
            destination="your-project.your_dataset.staging_table",
            job_config=job_config,
        )

    load_job.result()  # Wait for completion


    update_query = f"""
    UPDATE `{MAIN_TABLE}` AS main
    SET main.user_email = updates.user_email
    FROM `{STAGING_TABLE}` AS updates
    WHERE main.user_id = updates.user_id
    """

    update_task = BigQueryInsertJobOperator(
        task_id="update_main_table",
        configuration={
            "query": {
                "query": update_query,
                "useLegacySql": False,
            }
        },
        location="US",  # adjust if needed
    )

    upload_task >> update_task

</pre>
<pre>
from airflow import DAG
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from datetime import datetime

PROJECT_ID = "your-project"
DATASET = "your_dataset"
STAGING_TABLE = f"{PROJECT_ID}.{DATASET}.staging_table"
MAIN_TABLE = f"{PROJECT_ID}.{DATASET}.main_table"
GCS_BUCKET = "your-gcs-bucket"
GCS_OBJECT = "path/to/update_file.csv"  # e.g., "uploads/update_file.csv"

default_args = {
    "start_date": datetime(2024, 1, 1),
}

with DAG("update_user_email_from_gcs", default_args=default_args, schedule_interval=None, catchup=False) as dag:

    load_csv_from_gcs = GCSToBigQueryOperator(
        task_id="load_csv_to_staging_table",
        bucket=GCS_BUCKET,
        source_objects=[GCS_OBJECT],
        destination_project_dataset_table=STAGING_TABLE,
        schema_fields=[
            {"name": "user_id", "type": "STRING", "mode": "NULLABLE"},
            {"name": "user_email", "type": "STRING", "mode": "NULLABLE"},
        ],
        source_format="CSV",
        skip_leading_rows=1,
        write_disposition="WRITE_TRUNCATE",
        autodetect=False,
    )

    update_main_table = BigQueryInsertJobOperator(
        task_id="update_main_table",
        configuration={
            "query": {
                "query": f"""
                    UPDATE `{MAIN_TABLE}` AS main
                    SET main.user_email = updates.user_email
                    FROM `{STAGING_TABLE}` AS updates
                    WHERE main.user_id = updates.user_id
                """,
                "useLegacySql": False,
            }
        },
        location="US",  # Change if your dataset is in another region
    )

    load_csv_from_gcs >> update_main_table

</pre>
<pre>
from airflow.providers.google.cloud.sensors.gcs import GoogleCloudStorageObjectExistenceSensor
from datetime import timedelta

check_file_sensor = GoogleCloudStorageObjectExistenceSensor(
    task_id="check_gcs_file",
    bucket="your-bucket-name",
    object="path/to/your_file.csv",  # Adjust this path
    poke_interval=60,                # Check every 60 seconds
    timeout=3 * 60 * 60,             # 3 hours in seconds (10,800s)
    mode="poke",                     # or "reschedule" to reduce worker load
)

</pre>