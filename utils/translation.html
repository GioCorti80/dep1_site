<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>Translation</h1>
<h3>Config</h3>
<pre>
Hi Deeraj,  

Regarding the attached public key, I would need to know whether it has already been uploaded to the production Secret Manager or if it will be uploaded in the next release.  

Thank you!
</pre>
<pre>
Regarding the technical account svc_8960FTRE and change request CHG5445656,

In order to write to the network folder \dbg.db.com\res_res via SFTP Composer, we need to:

Create an alias to enable writing to the network folder

Address the issue where we currently encounter an error when attempting to establish an SFTP connection

Let me know if you need any refinements
</pre>

<pre>
Only after the previous DAG has finished running, trigger the following DAG: 125464-fsdfsd.
</pre>
<pre>
Hi Surendhar,

I just opened this SR (RITM54454) a little while ago to manually run the following DAGs:

Thank you!
</pre>

<pre>
Hi Igor,

Currently, the following folders are definitely not used by the transaction monitoring process:
/app.tr/uat
/app.tr/test

I still need to check the folders:
/app.tr/tmonitor (packed in a zip file)
/app.tr/bin

to confirm whether they are also unused by the process.
</pre>
<pre>
    Ok, thank you, Surendhar. You may now proceed with triggering DAG
</pre>
<pre>
    Hi Alexandru,

    The query performs a daily extraction directly from the t_trd_ana_ndg_glober table. Therefore, any changes to that table will impact the retrieved records.  Also, the number of records returned is affected by missing data in the UAT environment. There's no specific process for extracting only the delta."
</pre>
<pre>Yes, the scope of the position file is the same as that of the counterparty query.</pre>

<pre>
Hi Alexandru,

The issue we're seeing might be specific to the UAT environment, as the 'missing' field seems to be First_name.

In production, the person's first and last names are separated by a plus sign (+). However, in UAT, due to data masking, this separation doesn't occur, and the names appear as a single string. This could be why the split operation isn't working correctly.

We can modify the query to try and handle this specific scenario.
</pre>
<pre>
Hi Surendhar,
Yesterday I opened SR RTI353454 to request the pause of the following DAG:
124324 fdsfdsfds
We need it to be paused before the automatic run scheduled for tomorrow.
Could you please take care of it?
</pre>

<pre>
Hi Surendhar,
Yesterday I opened SR RIT5454 with a request to trigger the following DAG in production:
125974 fsfdsfs
Could you please proceed with this when possible?
</pre>
<pre>
Hi Alexandru,
I’ve updated the query to address the issue related to splitting the First_name and Last_name fields.
The file was sent this morning via DBExchange UAT and includes all 18 expected fields.
</pre>

<pre>
    The name you provided does not match any report I have managed.
    It's not a report that I was involved with.
</pre>
<pre>
Hi Igor,
I tried testing the new workflow, but it seems that the modified files in the repository are not being updated.
This issue occurs for both transaction-monitoring and app-reporting-python.
The modified files are not being reflected in the bucket.

For example:

For transaction-monitoring: gs://fsdfdsfsdds

For reporting-python: gs://fsdfdsfsdds
</pre>
<pre>
Hi Igor,
Alright, the branch for transaction-monitoring has already been merged into RELEASE_dbc_dev.
I’ve checked the files in the reporting-python bucket, and they’re now updated and aligned with the repository.
I’m now proceeding with the deployment of the transaction-monitoring dev branch.
</pre>
<pre>
Hi Janani,
I’ve prepared the following PR for reporting-engine, which includes updates to both report_list_engine and reporting_mandatory.xlsx.
It has now been approved for dev, and I’ve also raised the PR for UAT.
</pre>
<pre>
Hi Alexandru,
I'm attaching to this email the extraction from UAT, which contains some data.
We've also sent the GPG file via DBExchange.
</pre>
<pre>
Buongiorno
per poter eseguire l'implementazione su rotool della virtual-region il run del mese di aprile parità dopo la release prevista in data odierna.

Ciao.
</pre>
<pre>
In order to write via SFTP to the folder \\dbg\fsdsf\fgdf, we need to create an alias for the user svc_898989 that allows this technical user to write to the specified folder (ACL has already been assigned).
</pre>
<pre>
Could you please unpause the following DAG:

125479 Transaction Monitoring

The DAG should start automatically. If it doesn't, we kindly ask you to trigger it manually.
</pre>

<pre>
Hi Surendhar,
I've opened SR RIT2423432 to request the unpause of the following DAG:
125479 Transaction Monitoring.
It should start automatically, but if it doesn’t, please trigger it manually.
</pre>
<pre>
    Thanks, Surendhar. I’ll monitor the DAG this morning.
</pre>
<pre>
git è purtroppo un software indispensabile per poter operare i rilasci e viene utilizzato in locale sul pc
</pre>
<pre>
Hi Surendhar,
Could you kindly share the Dataproc log for ID432 fsdfdsfdsfsfdsfsfsdds?
Thanks!
</pre>
<pre>
ciao Alessandro,
non ho mai provato a recuperare file dalle cartelle di rete.
So che su alcune cartelle è possibile fare tasto-destro>proprietà e poi cliccare sul tab "versioni precedenti" però non sono sicuro che sia una funzione sempre attiva
</pre>
<pre>
Hi Surendhar,
In relation to SR RIT23423 for transaction-monitoring, could you please trigger the following DAG?
12354 dag-persistent
with the following parameters:

This is to attempt recovery of the transaction-monitoring fin_instr task that failed due to resource issues.
</pre>
<pre>
lo storico dei file dovrebbe essere ancora presente sul bucket di produzione al quale però non avviamo accesso direttamente ed è necessario ottenere un'autorizzazione per poter scaricare i file. Puoi provare a chiedere ad Antonio Sannino
</pre>
<pre>
OK, thanks Surendhar.
Could you please mark the fin task of the transaction-monitoring DAG as Succeeded, so the DAG can continue with the next tasks?
</pre>
<pre>
gsutil ls gs://your-bucket-name/path/to/directory/Posizioni_202505*
</pre>
<pre>
The current query isn’t set up to retrieve data from previous days. I believe the request is about the possibility of downloading files from earlier extractions already stored in the bucket.
</pre>
<pre>
The current query isn’t set up to retrieve data from previous days. I believe the request refers to downloading files from earlier extractions already stored in the bucket.
Here’s the list of files:
    
    file1
    
    file2
    
    file3
</pre>
<pre>
I’ve opened a support request to download these three Dataproc logs. Not sure if this is something I can ask you about—let me know.
Thanks!
</pre>
<pre>
Hi Surendhar,
I opened SR RIT1362323 to trigger the following DAG in production:

125479 controlli**
</pre>
<pre>
    Hi Hanumantha,
    We unpaused and triggered the DAG yesterday after the release, along with Surendhar. It can remain in the active state. I had paused the DAG earlier just to wait for the changes in the last release.
</pre>
<pre>
Hi Surendhar,
I’ve opened a support request (SR RIT434234) to trigger the following DAG:
qwerw-tm-sftp
</pre>
<pre>
Hi Laraib,

Following our previous call, I confirm the Sigma file was uploaded today, April 17, 2025. This was done after confirmation due to the initial health check failure.

Best regards,
Andrea
</pre>
<pre>
test1
    </pre>
<pre>
    Ciao Laura,
si tratta di un problema con la lettura del mapping specification
è stata aggiunta una riga che contiene la il testo b22 e pertanto c'è un problema nella lettura della tabelle specificatamente per questo report
Il problema non sarà risolvibile se non nella prossima release nel frattempo entro settimana prossima provo a verificare se riesco ad eseguire il run su awp ma il codice richiede un adattamento
Per evitare problemi in futuro
</pre>
<pre>
Dear [Recipient],
Please find attached the final report of the Sigma Transaction Monitoring process.
The process concluded with the following status: HEALTH CHECK FAILED – file not uploaded.
</pre>
<pre>Hi Sourav,
    I’ve finished now—feel free to proceed whenever you're ready.</pre>

<pre>
<<<<<<< HEAD
I'm Gio Corti, a developer with NAR 125479. I'm working with GCP, Python, and PySpark. Code Assist is helping me with app development.
</pre>
<pre>
Hi Enrico,

Please find attached the extracted JSON file.

Business date: 2025-04-22

Best regards,
Andrea
=======
Last year, DAG 125479 was created to upload tables to the Oracle staging area.
During the most recent run, it executed successfully, and I expect that the tables have been loaded.

The table b103_mm_priorira is no longer being updated, and the two models have been consolidated into the table b104_priorità.

The Assegni priorità table should have been populated correctly.
The tm_weekly_health_check is currently not included among the DAG’s tasks.

The DAG’s activity ends with loading the staging area and sending the OK files.
However, we do not have visibility into what happens within Oracle during the transition from the staging area to the Oracle tables. For this reason, I’ll need to follow up with Salvatore.
</pre>

<pre>
Good morning everyone,
Please find attached the file that was generated.
>>>>>>> bdb6237954e51497e4a3da8a36cbb308c4a0d4b5
</pre>
<pre>
We need to create an alias for the account svc_909809 so that it has write access to the following folder:
\\dbg.adasd\dsfgdgd\fsfds
</pre>
<pre>
    Hi Surendhar, I’m sorry, but I don’t believe I’ve been involved with this report in the past.
    Hi Surendhar, I’m afraid I haven’t worked on this report before.
</pre>
<pre>
Hi Snagram,
this isn’t a report related to transaction monitoring.

Thanks,
Andrea
</pre>
<pre>
Hi Hanumantha,
The job has run successfully.
Thank you.
</pre>
<pre>
Ciao Antonio Ho partecipato Ieri alla riunione di monitoring tool relativa però alla parte caricamenti 
Allo stato attuale ho accesso alla landing zone e al repository 
Da quello che ho capito La parte relativa alla reportistica dovrebbe partire dopo il caricamento dei databart 
Attualmente non mi è chiaro il meccanismo di deploit che viene utilizzato perché non vedo delle tag in composer 
Ho visto che sono presenti dalle Store procedurs i Bee query relativamente al caricamento delle tabelle 

attualmente non sono chiari i meccanismi di deploy
    
</pre>
<pre>
Hi Enrico,
Please find attached the output generated by the code.

Best regards
</pre>
<pre>
Hi everyone,
I’m running into an issue with these two PRs, specifically with the SLDC controls for the unit tests.
I keep getting an error: "Cache service responded with 422", even after trying several times.

Has anyone encountered this before or has any advice? Thanks in advance!
</pre>
<pre>
translate in english in natural and  clear way
Buongiorno,
si tratta di un report mensile previsto nella versione originaria del report.
Se non più necessario provvediamo a rimuoverlo.

Ciao.


Ciao Alberto,
volevo solo informari che settimana scorsa ho aperto la seguente CHG relativamente alla richiesta per la creazione di un alias per quanto riguarda il tecnical account svc_8956456 per poter scrivere sulla sottocartella sfdfsdfd.

Ciao e Grazie
</pre>
<pre>
    Ciao Alberto,
    in relazione alla change sai dirmi quale alias dell'account devo utilizzare?
    
</pre>
<pre>
Hi all,
I’m encountering an issue with the SDLC checks for UAT on the following two PRs:

pr1

pr2
Could you please take a look?
</pre>
<pre>
Ciao Andrea,
un rilascio per i test verrà eseguito nella prossima release del 08/05 il run è previsto per il 15 mentre per quanto riguarda le estrazioni dovrei eseguirle in giornata
</pre>

<pre>
The view reported in the incidents refers to the new models implemented in 2024, with the upload process having commenced at the end of 2024. The corresponding BigQuery tables are as follows:
</pre>
<pre>
The staging area is populated via the trigger of the following DAG:
</pre>
<pre>
The table TM_WEEKLY_HEALTH_CHECK is not part of the Oracle staging area upload process.

The table b103_mm_priorita is no longer populated as it has been merged into b103_priorita.

The DT_RIF_BUSINESS column is not present in the BigQuery tables, which are instead partitioned by the DATA_RIF field. This column is created directly by the Control-M process on Oracle.
</pre>
<pre>
Hi Hanumantha,

I've added comments to the two incidents regarding the issues users encountered with Oracle.
</pre>
<pre>
Hi Navend, I've attached the correct files.
Hi Navend, here are the updated/correct files for you.
</pre>
<pre>
Hi Sattwik,
Unfortunately, I haven’t developed any jobs or applications so far that involve connecting Dataproc to on-prem Hive.
The connection string you used looks correct.

This is the connection string I usually used (only on on-prem EAP):
jdbcs:88fsfdsfsd
However, as I mentioned, I’ve never developed jobs that read data from Hive EAP through GCP Dataproc.
</pre>
<pre>
Buongiorno a tutti,
la presente per comunicare che nella prossima release (run monthly del 05/2025) nella mail finale sarà aggiunta l'informazione relativa all'upload del file su dbexchange.
Di seguito gli esempi delle nuove mail inviate
</pre>
<pre>
Hey Surendhar,

I opened service request SR54545 a few days ago. Could you please put the following on hold until this month's release? This is to prevent it from running automatically.
</pre>
<pre>
On the development side, we can include the scheduling change in the next release on May 21st.
Currently, both reports are scheduled to run daily at 10 AM.
</pre>
<pre>
Hi Surendhar,
I’ve just opened service request RIT43243 to unpause the DAG 1235fd.
Could you please reactivate the DAG so it can run automatically today at 9 AM?
</pre>
<pre>
Ciao Antonio,
attualmente non ho ancora contattato Priyanka ho provato a fare alcuni test su log explorer per capire quali soluzioni potrebbero essere implementabili con l'utilizzo di python oppure soluzioni infrastrutturali nativa GCP con l'uso dei log_sink per l'esportazione dei log
A questo punto sarebbe utile schedulare una call per capire quale soluzione hanno scelto e che tipo di filtri inserire nella query su cloud log
</pre>
<pre>
Ok, thank you.
I was running some tests on Cloud Logging.
Do you have the name of the service account and the filters I should use to retrieve the relevant data from Cloud Logging?
Are the logs related to the project dretre-gdgfdgdf?

As a final output, are you expecting a BigQuery table or a file? And what information should we extract — only the queries, or is there additional data you would like us to retrieve?
</pre>
<pre>
Today, the monthly process ran: the table assegni_priorita was updated via the staging table stg_tm_priorita, and the file pwcfsfds.ok was sent.
</pre>
<pre>
Creating the development space is required to use the Assist Code Gemini application as a VS Code extension (as per the instructions at the following link).
</pre>
<pre>
Hi Prianka,

Okay, I'll be on today's call.
</pre>

<pre>
Hi Hanumantha,
Yes, thank you. The end user would like the extraction to be relaunched using the new query after the release.
</pre>
<pre>
Hi Laraib,
the JSON file containing the alerts was uploaded on May 9, 2025, and should have been visible starting the following day, May 10, 2025.
</pre>
<pre>
Hi all,
I can’t join the meeting this morning due to mandatory training, I’m continuing with my tasks.
</pre>
<pre>
Hi Alexandru,
the DAGs are currently scheduled at 10 AM UTC with no dependencies on other pipelines.
From the next release, they’ll run at 10 AM UTC for counterparty and 11 AM UTC for position.
</pre>
<pre>
Hi Hanumantha,
For the TM process, we load data into the stg_tm_assegni_priorita table in the Oracle staging area.
</pre>
<pre>
The DAG is triggered by the Transaction Monitoring DAG, which has the triggering of the tm_oracle DAG as its final task.
</pre>
<pre>
Hi Tarun,
It’s not a process I’m familiar with. It seems more like something that falls under the responsibilities of a Business Analyst or someone involved in DWH table
</pre>
<pre>
Hi Alexandru,
The schedule you indicated works for me, so the DAGs will be scheduled as follows:

10 AM UTC for counterparty

11 AM UTC for position
</pre>
<pre>
Ciao Luca,
potrebbe essere schedulato per la prossima release che potrebbe essere all'inizio di giugno.
Trattandosi di un invio su cartella di rete bisognerebbe verificare se esistono già altri report gcp che vengono inviati a tale cartella.
In caso affermativo l'implementazione è veloce.
In caso contrario bisognerà verificare che esista già un account tecnico che può scrivere sulla cartella di rete (anche se non collegato a gcp) altrimenti bisogna crearlo. In quest'ultimo caso i tempi si potrebbero allungare.

Vedo però che la cartella è la stessa dell'id1901 e altri report quindi non è necessario fare altre operazioni.
Non sono sicuro che siamo ancora in tempo per la release del 21/05 pertanto potrebbe andare nella release di inizio giugno.

Ti faccio sapere.

Ciao.
</pre>
<pre>
Giving this function and these paramters Hi wanna know if you think that:
the given condition -w (where) and the condition -b (business_date)
produce that the job (airflow dag) is going to extract all the record in the given table (-p idenficativo is a unique identifier)
</pre>
<pre>
To investigate the issue in production, I need to manually trigger the following DAG:
TM_ORACLE_CUSTOM with the parameters:

json
Copia
Modifica
{
  "param1": "2024-05",
  "param2": "de"
}
I’ve submitted service request RITM45454545 to the operations team with this request.
</pre>
<pre>
Thanks, Surendhar.
The load of the single table stg_assegni_priorita completed successfully.
@Tarun, could you please check if the staging table has been correctly populated with approximately 18,000 rows for the specified business date?

Here’s a summary of the parameters:

Table: stg_tm_assegni_priorita

File OK: PWCC_TR_FDFSFD
</pre>
<pre>
Planned release of the new version on 21-05-2025.
</pre>
<pre>
GEMINI
Creating the dev folder is required to use the Assist Code Gemini application as a VS Code extension, as outlined in the instructions at the following link.
</pre>
<pre>
The recovery of missing data in the tables will be carried out in the May 21 release by manually triggering DAG `125479-4-TM-ORACLE`.
</pre>
<pre>
To upload the file to the designated folder, we require the appropriate alias.
</pre>
<pre>
Ciao Antonio,
alla fine si è trattato di un incontro di knowledge sharing dove Scott ci ha mostrato una soluzione da loro adottata per il monitoraggio dei costi.
Attualmente loro utilizzano una tabella statica per il mapping tra user_id e e-mail che aggiornano con cadenza periodica.
Pryanka gli ha spiegato il nostro use case è legato ad un tema di sicurezza e che il nostro problema è quello di dover mantenere la tabella di mapping.
Sembrebbe possibile però inserire una label/tag nei log che includa anche l'e-mail oltre all'user_id in modo che non sia necessaria una tabella di raccordo. Si tratta però di una richiesta che andrebbe discussa con il team di looker.
</pre>
<pre>
Could you please trigger the DAG "125479-4-REPORT_PY_COUNT_OPE" twice? The parameters for each run are as follows:

First run:

JSON

{"param": "N", "dt_inizio": "2025-01"}
Second run:

JSON

{"param": "N", "dt_inizio": "2025-02"}
</pre>
<pre>
Hi Surendhar,

I've raised service request RITM45454545 to trigger the following DAG in production:

DAG Name: 125479-4 countope

Parameters:

JSON

{"param": "N", "dt_inizio": "2025-02"}
</pre>
<pre>
Thank you, Surendhar,
Since the run was successful, I kindly ask if you could trigger it one last time with the following parameters:
125479-4-REPORT_PY_COUNT_OPE

json
Copia
Modifica
{"param": "N",  
"dt_inizio": "2025-01"}
The request is within the same SR, which can be closed once this is completed.
Thank you.
</pre>
<pre>
da quello che è emerso dalla call di ieri sembra che l'informazione relativa alla user_email riferita all'user_id non sia nativamente presente nel log looker e anche il progetto che ci hanno presentato utilizzava una taella di transcodifica tra user_id/mail che viene aggiornata settimanalmente.
Hanno però detto che sarebbe possibile aggiungere anche il dato relativo alla mail ma è necessario fare una modifica lato google/looker per aggiungere la label/tag relativa alla user_email in modo da avere tutte le informazioni.

Se la soluzione di integrare i log di google con l'informazione della mail non sarà percorribile in tempi brevi bisognarà procedere con la modalità pensata inizialmente che prevere la creazione di una tabella di raccordo da mettere in join con i log in modo da avere le info necessarie (timestamp, quary_eseguita, user_mail, user_id)

Nel meeting non è stato trattato il tema del service-account necessario per la scrittura della tabella direttamente sul progetto 

</pre>
<pre>
ciao Antonio,
il run dei 2 script si è concluso correttamente
I 2 file vengono inviati un di seguito all'altro mentre il primo file quello _ctrl.csv per il secondo la dag ha eseguito 2 tentativi un a distanza di 1hr (primo tentativo non andato a buon fine mentre il secondo ok).
Questo per entrambi gli script.
Pertando le tempistiche sono risultate essere le seguenti:
Counterparty:
dag_start: 10 AM utc
file 1
file 2

Position:
dag_start: 11 AM utc
file 1
file 2
</pre>
<pre>
Hi Tarun,
Today the DAG for the full load of the Oracle tables related to the Transaction Monitoring project was run.
Could you please check if the DWH tables were loaded correctly?
</pre>
<pre>
Hi Alexandru,
We need to apply the same change on our side of the DAG in the upcoming release on June 5th, due to an unexpected issue with the DBExchange servers.
Regarding the usual internal folder, it's simply about delivering the report to a Deutsche Bank internal network folder for internal use.
</pre>
<pre>
Hi Surendhar,
The DAG 125479-estraz_cus failed this morning.
I've opened SR 545464654.
Could you please re-trigger the DAG?

Thanks a lot,
Best regards,
Bye
</pre>
<pre>
Hi Surendhar,
I see that the Spark session isn’t starting due to infrastructure issues.
Would it be possible to mark the previous run as succeeded and try triggering the DAG again from scratch, to check if the issue has been resolved?

Thanks!
</pre>
<pre>
Normally, it only takes a few minutes, but I see that the job is currently stuck in the queued state.
</pre>
<pre>
The DAG has failed again. I'll wait for the next scheduled run tomorrow morning to see if the issue persists.
</pre>
<pre>
I triggered the DAG in production, and the results are pending insertion into the Oracle DWH.
</pre>
<pre>
Task details:

Load the attached public key into the secret named public_key_glober_secret.
</pre>
<pre>
Yes, Sangram, I'm starting the task today.
</pre>
<pre>
We are experiencing some issues with sending files via SFTP to dbexchange.
In many cases, the transfers fail and multiple attempts are needed.
Could you please let us know what parameters or guidelines we should follow to reduce the number of failed transfers?
</pre>
<pre>
Hi Surendhar,

The DAG 125479-4-REPORT_PY_EVENT_DRIVEN failed this morning at 12:00.
I see that the Composer environment is now up and running.

Could you please trigger the DAG again?

I've opened a support request: SR RIT45454545.

Thanks a lot!
</pre>
<pre>
Effettivamente la versione rilasciata in produzione contiene ancora il filtro solo per gli estinti.
Inoltro in allegato l'estrazione eseguita manualmente con la rimozione del filtro
</pre>
<pre>
si tratta di tabelle piccole che in accordo con Antonio per ora continuiamo ad aggiornare su oracle (forse per ancora qualche run mensile). Verranno poi migrate
</pre>
<pre>
Hi Tarun,
I’ve requested the trigger of DAG 125479-TM-ORACLE to restart the loading process.
</pre>
<pre>
Hi Tarun,
there are some filtering issues with the weekly_tm table in the Oracle DAG, due to a different partitioning column. This will be fixed in the next release.

Regarding the vw_fact_tm_weekly_health_check table,
could you please confirm the name of the staging table and the correct naming of the expected .ok file, so I can include them in the next release as well?
</pre>
<pre>
Is there another table that contains the word weekly, like stg_tm_weekly_health?
Because Genovese also mentioned the table vw_fact_weekly_health_check in the incident, but it wasn’t included in the initial scope that had been provided.
</pre>
<pre>
Hi all,
I’d like to ask if the correct naming for the "OK" file for the table is the following:
pwcc_tm_transaction
</pre>