<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Black Background with White Text</title>
    <style>
        body {
            background-color: black;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
        }
    </style>
</head>
<h1>Translation</h1>
<h3>Config</h3>
<pre>
Hi Deeraj,  

Regarding the attached public key, I would need to know whether it has already been uploaded to the production Secret Manager or if it will be uploaded in the next release.  

Thank you!
</pre>
<pre>
Regarding the technical account svc_8960FTRE and change request CHG5445656,

In order to write to the network folder \dbg.db.com\res_res via SFTP Composer, we need to:

Create an alias to enable writing to the network folder

Address the issue where we currently encounter an error when attempting to establish an SFTP connection

Let me know if you need any refinements
</pre>

<pre>
Only after the previous DAG has finished running, trigger the following DAG: 125464-fsdfsd.
</pre>
<pre>
Hi Surendhar,

I just opened this SR (RITM54454) a little while ago to manually run the following DAGs:

Thank you!
</pre>

<pre>
Hi Igor,

Currently, the following folders are definitely not used by the transaction monitoring process:
/app.tr/uat
/app.tr/test

I still need to check the folders:
/app.tr/tmonitor (packed in a zip file)
/app.tr/bin

to confirm whether they are also unused by the process.
</pre>
<pre>
    Ok, thank you, Surendhar. You may now proceed with triggering DAG
</pre>
<pre>
    Hi Alexandru,

    The query performs a daily extraction directly from the t_trd_ana_ndg_glober table. Therefore, any changes to that table will impact the retrieved records.  Also, the number of records returned is affected by missing data in the UAT environment. There's no specific process for extracting only the delta."
</pre>
<pre>Yes, the scope of the position file is the same as that of the counterparty query.</pre>

<pre>
Hi Alexandru,

The issue we're seeing might be specific to the UAT environment, as the 'missing' field seems to be First_name.

In production, the person's first and last names are separated by a plus sign (+). However, in UAT, due to data masking, this separation doesn't occur, and the names appear as a single string. This could be why the split operation isn't working correctly.

We can modify the query to try and handle this specific scenario.
</pre>
<pre>
Hi Surendhar,
Yesterday I opened SR RTI353454 to request the pause of the following DAG:
124324 fdsfdsfds
We need it to be paused before the automatic run scheduled for tomorrow.
Could you please take care of it?
</pre>

<pre>
Hi Surendhar,
Yesterday I opened SR RIT5454 with a request to trigger the following DAG in production:
125974 fsfdsfs
Could you please proceed with this when possible?
</pre>
<pre>
Hi Alexandru,
I’ve updated the query to address the issue related to splitting the First_name and Last_name fields.
The file was sent this morning via DBExchange UAT and includes all 18 expected fields.
</pre>

<pre>
    The name you provided does not match any report I have managed.
    It's not a report that I was involved with.
</pre>
<pre>
Hi Igor,
I tried testing the new workflow, but it seems that the modified files in the repository are not being updated.
This issue occurs for both transaction-monitoring and app-reporting-python.
The modified files are not being reflected in the bucket.

For example:

For transaction-monitoring: gs://fsdfdsfsdds

For reporting-python: gs://fsdfdsfsdds
</pre>
<pre>
Hi Igor,
Alright, the branch for transaction-monitoring has already been merged into RELEASE_dbc_dev.
I’ve checked the files in the reporting-python bucket, and they’re now updated and aligned with the repository.
I’m now proceeding with the deployment of the transaction-monitoring dev branch.
</pre>
<pre>
Hi Janani,
I’ve prepared the following PR for reporting-engine, which includes updates to both report_list_engine and reporting_mandatory.xlsx.
It has now been approved for dev, and I’ve also raised the PR for UAT.
</pre>
<pre>
Hi Alexandru,
I'm attaching to this email the extraction from UAT, which contains some data.
We've also sent the GPG file via DBExchange.
</pre>
<pre>
Buongiorno
per poter eseguire l'implementazione su rotool della virtual-region il run del mese di aprile parità dopo la release prevista in data odierna.

Ciao.
</pre>
<pre>
In order to write via SFTP to the folder \\dbg\fsdsf\fgdf, we need to create an alias for the user svc_898989 that allows this technical user to write to the specified folder (ACL has already been assigned).
</pre>
<pre>
Could you please unpause the following DAG:

125479 Transaction Monitoring

The DAG should start automatically. If it doesn't, we kindly ask you to trigger it manually.
</pre>

<pre>
Hi Surendhar,
I've opened SR RIT2423432 to request the unpause of the following DAG:
125479 Transaction Monitoring.
It should start automatically, but if it doesn’t, please trigger it manually.
</pre>
<pre>
    Thanks, Surendhar. I’ll monitor the DAG this morning.
</pre>
<pre>
git è purtroppo un software indispensabile per poter operare i rilasci e viene utilizzato in locale sul pc
</pre>
<pre>
Hi Surendhar,
Could you kindly share the Dataproc log for ID432 fsdfdsfdsfsfdsfsfsdds?
Thanks!
</pre>
<pre>
ciao Alessandro,
non ho mai provato a recuperare file dalle cartelle di rete.
So che su alcune cartelle è possibile fare tasto-destro>proprietà e poi cliccare sul tab "versioni precedenti" però non sono sicuro che sia una funzione sempre attiva
</pre>
<pre>
Hi Surendhar,
In relation to SR RIT23423 for transaction-monitoring, could you please trigger the following DAG?
12354 dag-persistent
with the following parameters:

This is to attempt recovery of the transaction-monitoring fin_instr task that failed due to resource issues.
</pre>
<pre>
lo storico dei file dovrebbe essere ancora presente sul bucket di produzione al quale però non avviamo accesso direttamente ed è necessario ottenere un'autorizzazione per poter scaricare i file. Puoi provare a chiedere ad Antonio Sannino
</pre>
<pre>
OK, thanks Surendhar.
Could you please mark the fin task of the transaction-monitoring DAG as Succeeded, so the DAG can continue with the next tasks?
</pre>
<pre>
gsutil ls gs://your-bucket-name/path/to/directory/Posizioni_202505*
</pre>
<pre>
The current query isn’t set up to retrieve data from previous days. I believe the request is about the possibility of downloading files from earlier extractions already stored in the bucket.
</pre>
<pre>
The current query isn’t set up to retrieve data from previous days. I believe the request refers to downloading files from earlier extractions already stored in the bucket.
Here’s the list of files:
    
    file1
    
    file2
    
    file3
</pre>
<pre>
I’ve opened a support request to download these three Dataproc logs. Not sure if this is something I can ask you about—let me know.
Thanks!
</pre>
<pre>
Hi Surendhar,
I opened SR RIT1362323 to trigger the following DAG in production:

125479 controlli**
</pre>
<pre>
    Hi Hanumantha,
    We unpaused and triggered the DAG yesterday after the release, along with Surendhar. It can remain in the active state. I had paused the DAG earlier just to wait for the changes in the last release.
</pre>
<pre>
Hi Surendhar,
I’ve opened a support request (SR RIT434234) to trigger the following DAG:
qwerw-tm-sftp
</pre>
<pre>
Hi Laraib,

Following our previous call, I confirm the Sigma file was uploaded today, April 17, 2025. This was done after confirmation due to the initial health check failure.

Best regards,
Andrea
</pre>
<pre>
test1
    </pre>
<pre>
    Ciao Laura,
si tratta di un problema con la lettura del mapping specification
è stata aggiunta una riga che contiene la il testo b22 e pertanto c'è un problema nella lettura della tabelle specificatamente per questo report
Il problema non sarà risolvibile se non nella prossima release nel frattempo entro settimana prossima provo a verificare se riesco ad eseguire il run su awp ma il codice richiede un adattamento
Per evitare problemi in futuro
</pre>
<pre>
Dear [Recipient],
Please find attached the final report of the Sigma Transaction Monitoring process.
The process concluded with the following status: HEALTH CHECK FAILED – file not uploaded.
</pre>
<pre>Hi Sourav,
    I’ve finished now—feel free to proceed whenever you're ready.</pre>

<pre>
<<<<<<< HEAD
I'm Gio Corti, a developer with NAR 125479. I'm working with GCP, Python, and PySpark. Code Assist is helping me with app development.
</pre>
<pre>
Hi Enrico,

Please find attached the extracted JSON file.

Business date: 2025-04-22

Best regards,
Andrea
=======
Last year, DAG 125479 was created to upload tables to the Oracle staging area.
During the most recent run, it executed successfully, and I expect that the tables have been loaded.

The table b103_mm_priorira is no longer being updated, and the two models have been consolidated into the table b104_priorità.

The Assegni priorità table should have been populated correctly.
The tm_weekly_health_check is currently not included among the DAG’s tasks.

The DAG’s activity ends with loading the staging area and sending the OK files.
However, we do not have visibility into what happens within Oracle during the transition from the staging area to the Oracle tables. For this reason, I’ll need to follow up with Salvatore.
</pre>

<pre>
Good morning everyone,
Please find attached the file that was generated.
>>>>>>> bdb6237954e51497e4a3da8a36cbb308c4a0d4b5
</pre>
<pre>
We need to create an alias for the account svc_909809 so that it has write access to the following folder:
\\dbg.adasd\dsfgdgd\fsfds
</pre>
<pre>
    Hi Surendhar, I’m sorry, but I don’t believe I’ve been involved with this report in the past.
    Hi Surendhar, I’m afraid I haven’t worked on this report before.
</pre>
<pre>
Hi Snagram,
this isn’t a report related to transaction monitoring.

Thanks,
Andrea
</pre>
<pre>
Hi Hanumantha,
The job has run successfully.
Thank you.
</pre>
<pre>
Ciao Antonio Ho partecipato Ieri alla riunione di monitoring tool relativa però alla parte caricamenti 
Allo stato attuale ho accesso alla landing zone e al repository 
Da quello che ho capito La parte relativa alla reportistica dovrebbe partire dopo il caricamento dei databart 
Attualmente non mi è chiaro il meccanismo di deploit che viene utilizzato perché non vedo delle tag in composer 
Ho visto che sono presenti dalle Store procedurs i Bee query relativamente al caricamento delle tabelle 

attualmente non sono chiari i meccanismi di deploy
    
</pre>
<pre>
Hi Enrico,
Please find attached the output generated by the code.

Best regards
</pre>
<pre>
Hi everyone,
I’m running into an issue with these two PRs, specifically with the SLDC controls for the unit tests.
I keep getting an error: "Cache service responded with 422", even after trying several times.

Has anyone encountered this before or has any advice? Thanks in advance!
</pre>
<pre>
translate in english in natural and  clear way
Buongiorno,
si tratta di un report mensile previsto nella versione originaria del report.
Se non più necessario provvediamo a rimuoverlo.

Ciao.


Ciao Alberto,
volevo solo informari che settimana scorsa ho aperto la seguente CHG relativamente alla richiesta per la creazione di un alias per quanto riguarda il tecnical account svc_8956456 per poter scrivere sulla sottocartella sfdfsdfd.

Ciao e Grazie
</pre>
<pre>
    Ciao Alberto,
    in relazione alla change sai dirmi quale alias dell'account devo utilizzare?
    
</pre>
<pre>
Hi all,
I’m encountering an issue with the SDLC checks for UAT on the following two PRs:

pr1

pr2
Could you please take a look?
</pre>
<pre>
Ciao Andrea,
un rilascio per i test verrà eseguito nella prossima release del 08/05 il run è previsto per il 15 mentre per quanto riguarda le estrazioni dovrei eseguirle in giornata
</pre>

<pre>
The view reported in the incidents refers to the new models implemented in 2024, with the upload process having commenced at the end of 2024. The corresponding BigQuery tables are as follows:
</pre>
<pre>
The staging area is populated via the trigger of the following DAG:
</pre>
<pre>
The table TM_WEEKLY_HEALTH_CHECK is not part of the Oracle staging area upload process.

The table b103_mm_priorita is no longer populated as it has been merged into b103_priorita.

The DT_RIF_BUSINESS column is not present in the BigQuery tables, which are instead partitioned by the DATA_RIF field. This column is created directly by the Control-M process on Oracle.
</pre>
<pre>
Hi Hanumantha,

I've added comments to the two incidents regarding the issues users encountered with Oracle.
</pre>
<pre>
Hi Navend, I've attached the correct files.
Hi Navend, here are the updated/correct files for you.
</pre>
<pre>
Hi Sattwik,
Unfortunately, I haven’t developed any jobs or applications so far that involve connecting Dataproc to on-prem Hive.
The connection string you used looks correct.

This is the connection string I usually used (only on on-prem EAP):
jdbcs:88fsfdsfsd
However, as I mentioned, I’ve never developed jobs that read data from Hive EAP through GCP Dataproc.
</pre>
<pre>
Buongiorno a tutti,
la presente per comunicare che nella prossima release (run monthly del 05/2025) nella mail finale sarà aggiunta l'informazione relativa all'upload del file su dbexchange.
Di seguito gli esempi delle nuove mail inviate
</pre>
<pre>
Hey Surendhar,

I opened service request SR54545 a few days ago. Could you please put the following on hold until this month's release? This is to prevent it from running automatically.
</pre>
<pre>
On the development side, we can include the scheduling change in the next release on May 21st.
Currently, both reports are scheduled to run daily at 10 AM.
</pre>
<pre>
Hi Surendhar,
I’ve just opened service request RIT43243 to unpause the DAG 1235fd.
Could you please reactivate the DAG so it can run automatically today at 9 AM?
</pre>
<pre>
Ciao Antonio,
attualmente non ho ancora contattato Priyanka ho provato a fare alcuni test su log explorer per capire quali soluzioni potrebbero essere implementabili con l'utilizzo di python oppure soluzioni infrastrutturali nativa GCP con l'uso dei log_sink per l'esportazione dei log
A questo punto sarebbe utile schedulare una call per capire quale soluzione hanno scelto e che tipo di filtri inserire nella query su cloud log
</pre>
<pre>
Ok, thank you.
I was running some tests on Cloud Logging.
Do you have the name of the service account and the filters I should use to retrieve the relevant data from Cloud Logging?
Are the logs related to the project dretre-gdgfdgdf?

As a final output, are you expecting a BigQuery table or a file? And what information should we extract — only the queries, or is there additional data you would like us to retrieve?
</pre>
<pre>
Today, the monthly process ran: the table assegni_priorita was updated via the staging table stg_tm_priorita, and the file pwcfsfds.ok was sent.
</pre>
<pre>
Creating the development space is required to use the Assist Code Gemini application as a VS Code extension (as per the instructions at the following link).
</pre>
<pre>
Hi Prianka,

Okay, I'll be on today's call.
</pre>

<pre>
Hi Hanumantha,
Yes, thank you. The end user would like the extraction to be relaunched using the new query after the release.
</pre>
<pre>
Hi Laraib,
the JSON file containing the alerts was uploaded on May 9, 2025, and should have been visible starting the following day, May 10, 2025.
</pre>
<pre>
Hi all,
I can’t join the meeting this morning due to mandatory training, I’m continuing with my tasks.
</pre>
<pre>
Hi Alexandru,
the DAGs are currently scheduled at 10 AM UTC with no dependencies on other pipelines.
From the next release, they’ll run at 10 AM UTC for counterparty and 11 AM UTC for position.
</pre>
<pre>
Hi Hanumantha,
For the TM process, we load data into the stg_tm_assegni_priorita table in the Oracle staging area.
</pre>
<pre>
The DAG is triggered by the Transaction Monitoring DAG, which has the triggering of the tm_oracle DAG as its final task.
</pre>
<pre>
Hi Tarun,
It’s not a process I’m familiar with. It seems more like something that falls under the responsibilities of a Business Analyst or someone involved in DWH table
</pre>
<pre>
Hi Alexandru,
The schedule you indicated works for me, so the DAGs will be scheduled as follows:

10 AM UTC for counterparty

11 AM UTC for position
</pre>
<pre>
Ciao Luca,
potrebbe essere schedulato per la prossima release che potrebbe essere all'inizio di giugno.
Trattandosi di un invio su cartella di rete bisognerebbe verificare se esistono già altri report gcp che vengono inviati a tale cartella.
In caso affermativo l'implementazione è veloce.
In caso contrario bisognerà verificare che esista già un account tecnico che può scrivere sulla cartella di rete (anche se non collegato a gcp) altrimenti bisogna crearlo. In quest'ultimo caso i tempi si potrebbero allungare.

Vedo però che la cartella è la stessa dell'id1901 e altri report quindi non è necessario fare altre operazioni.
Non sono sicuro che siamo ancora in tempo per la release del 21/05 pertanto potrebbe andare nella release di inizio giugno.

Ti faccio sapere.

Ciao.
</pre>
<pre>
Giving this function and these paramters Hi wanna know if you think that:
the given condition -w (where) and the condition -b (business_date)
produce that the job (airflow dag) is going to extract all the record in the given table (-p idenficativo is a unique identifier)
</pre>
<pre>
To investigate the issue in production, I need to manually trigger the following DAG:
TM_ORACLE_CUSTOM with the parameters:

json
Copia
Modifica
{
  "param1": "2024-05",
  "param2": "de"
}
I’ve submitted service request RITM45454545 to the operations team with this request.
</pre>
<pre>
Thanks, Surendhar.
The load of the single table stg_assegni_priorita completed successfully.
@Tarun, could you please check if the staging table has been correctly populated with approximately 18,000 rows for the specified business date?

Here’s a summary of the parameters:

Table: stg_tm_assegni_priorita

File OK: PWCC_TR_FDFSFD
</pre>
<pre>
Planned release of the new version on 21-05-2025.
</pre>
<pre>
GEMINI
Creating the dev folder is required to use the Assist Code Gemini application as a VS Code extension, as outlined in the instructions at the following link.
</pre>
<pre>
The recovery of missing data in the tables will be carried out in the May 21 release by manually triggering DAG `125479-4-TM-ORACLE`.
</pre>
<pre>
To upload the file to the designated folder, we require the appropriate alias.
</pre>
<pre>
Ciao Antonio,
alla fine si è trattato di un incontro di knowledge sharing dove Scott ci ha mostrato una soluzione da loro adottata per il monitoraggio dei costi.
Attualmente loro utilizzano una tabella statica per il mapping tra user_id e e-mail che aggiornano con cadenza periodica.
Pryanka gli ha spiegato il nostro use case è legato ad un tema di sicurezza e che il nostro problema è quello di dover mantenere la tabella di mapping.
Sembrebbe possibile però inserire una label/tag nei log che includa anche l'e-mail oltre all'user_id in modo che non sia necessaria una tabella di raccordo. Si tratta però di una richiesta che andrebbe discussa con il team di looker.
</pre>
<pre>
Could you please trigger the DAG "125479-4-REPORT_PY_COUNT_OPE" twice? The parameters for each run are as follows:

First run:

JSON

{"param": "N", "dt_inizio": "2025-01"}
Second run:

JSON

{"param": "N", "dt_inizio": "2025-02"}
</pre>
<pre>
Hi Surendhar,

I've raised service request RITM45454545 to trigger the following DAG in production:

DAG Name: 125479-4 countope

Parameters:

JSON

{"param": "N", "dt_inizio": "2025-02"}
</pre>
<pre>
Thank you, Surendhar,
Since the run was successful, I kindly ask if you could trigger it one last time with the following parameters:
125479-4-REPORT_PY_COUNT_OPE

json
Copia
Modifica
{"param": "N",  
"dt_inizio": "2025-01"}
The request is within the same SR, which can be closed once this is completed.
Thank you.
</pre>
<pre>
da quello che è emerso dalla call di ieri sembra che l'informazione relativa alla user_email riferita all'user_id non sia nativamente presente nel log looker e anche il progetto che ci hanno presentato utilizzava una taella di transcodifica tra user_id/mail che viene aggiornata settimanalmente.
Hanno però detto che sarebbe possibile aggiungere anche il dato relativo alla mail ma è necessario fare una modifica lato google/looker per aggiungere la label/tag relativa alla user_email in modo da avere tutte le informazioni.

Se la soluzione di integrare i log di google con l'informazione della mail non sarà percorribile in tempi brevi bisognarà procedere con la modalità pensata inizialmente che prevere la creazione di una tabella di raccordo da mettere in join con i log in modo da avere le info necessarie (timestamp, quary_eseguita, user_mail, user_id)

Nel meeting non è stato trattato il tema del service-account necessario per la scrittura della tabella direttamente sul progetto 

</pre>
<pre>
ciao Antonio,
il run dei 2 script si è concluso correttamente
I 2 file vengono inviati un di seguito all'altro mentre il primo file quello _ctrl.csv per il secondo la dag ha eseguito 2 tentativi un a distanza di 1hr (primo tentativo non andato a buon fine mentre il secondo ok).
Questo per entrambi gli script.
Pertando le tempistiche sono risultate essere le seguenti:
Counterparty:
dag_start: 10 AM utc
file 1
file 2

Position:
dag_start: 11 AM utc
file 1
file 2
</pre>
<pre>
Hi Tarun,
Today the DAG for the full load of the Oracle tables related to the Transaction Monitoring project was run.
Could you please check if the DWH tables were loaded correctly?
</pre>
<pre>
Hi Alexandru,
We need to apply the same change on our side of the DAG in the upcoming release on June 5th, due to an unexpected issue with the DBExchange servers.
Regarding the usual internal folder, it's simply about delivering the report to a Deutsche Bank internal network folder for internal use.
</pre>
<pre>
Hi Surendhar,
The DAG 125479-estraz_cus failed this morning.
I've opened SR 545464654.
Could you please re-trigger the DAG?

Thanks a lot,
Best regards,
Bye
</pre>
<pre>
Hi Surendhar,
I see that the Spark session isn’t starting due to infrastructure issues.
Would it be possible to mark the previous run as succeeded and try triggering the DAG again from scratch, to check if the issue has been resolved?

Thanks!
</pre>
<pre>
Normally, it only takes a few minutes, but I see that the job is currently stuck in the queued state.
</pre>
<pre>
The DAG has failed again. I'll wait for the next scheduled run tomorrow morning to see if the issue persists.
</pre>
<pre>
I triggered the DAG in production, and the results are pending insertion into the Oracle DWH.
</pre>
<pre>
Task details:

Load the attached public key into the secret named public_key_glober_secret.
</pre>
<pre>
Yes, Sangram, I'm starting the task today.
</pre>
<pre>
We are experiencing some issues with sending files via SFTP to dbexchange.
In many cases, the transfers fail and multiple attempts are needed.
Could you please let us know what parameters or guidelines we should follow to reduce the number of failed transfers?
</pre>
<pre>
Hi Surendhar,

The DAG 125479-4-REPORT_PY_EVENT_DRIVEN failed this morning at 12:00.
I see that the Composer environment is now up and running.

Could you please trigger the DAG again?

I've opened a support request: SR RIT45454545.

Thanks a lot!
</pre>
<pre>
Effettivamente la versione rilasciata in produzione contiene ancora il filtro solo per gli estinti.
Inoltro in allegato l'estrazione eseguita manualmente con la rimozione del filtro
</pre>
<pre>
si tratta di tabelle piccole che in accordo con Antonio per ora continuiamo ad aggiornare su oracle (forse per ancora qualche run mensile). Verranno poi migrate
</pre>
<pre>
Hi Tarun,
I’ve requested the trigger of DAG 125479-TM-ORACLE to restart the loading process.
</pre>
<pre>
Hi Tarun,
there are some filtering issues with the weekly_tm table in the Oracle DAG, due to a different partitioning column. This will be fixed in the next release.

Regarding the vw_fact_tm_weekly_health_check table,
could you please confirm the name of the staging table and the correct naming of the expected .ok file, so I can include them in the next release as well?
</pre>
<pre>
Is there another table that contains the word weekly, like stg_tm_weekly_health?
Because Genovese also mentioned the table vw_fact_weekly_health_check in the incident, but it wasn’t included in the initial scope that had been provided.
</pre>
<pre>
Hi all,
I’d like to ask if the correct naming for the "OK" file for the table is the following:
pwcc_tm_transaction
</pre>
<pre>
Hi all,
I’d like to confirm if the correct naming for the "OK" file that triggers the Control-M condition for the VM_HEALTH_TM table is:
PWCC_TRANSACTION_TM_WEEKLY_HEALTH_CHECK_
</pre>
<pre>
Hello everyone,
Could you please confirm if the correct name of the "OK" file that triggers the Control-M condition for the VM_HEALTH_TM table is:
PWCC_TRANSACTION_TM_WEEKLY_HEALTH_CHECK_?
</pre>
<pre>
translate in english in a professional and clear way
Hi Antonio,

Based on the list of reports generated and sent via SFTP, I couldn't find any CSV file named dg_pc_001.csv.
As per the most recent run, the credit card DAG produces the following files:
- dg_pc_001a.csv
- dg_pc_001b.csv
- dg_pc_001c.csv
- dg_pc_001e.csv
Unfortunately, I'm unable to view the SFTP task log for March related to the DAG 125479-carte_Dicredito (maybe due to log retention).
However, looking at the log for April (may run), it shows the following files were sent:
- 001a → 001a.csv
- 001a → 001b.csv
- 001a → 001c.csv

It might be worth checking on the SAS side whether the files were received correctly and whether any of them were discarded.
</pre>
<pre>
ciao Enrico,
l'operazione si potrebbe fare con python inserendo le variabili (dipende da che tipo variabili si tratta).
Con l'sql penso si possano creare delle tabelle intermedie con i dati da utilizzare.
In entrambi i casi andrebbe poi creato un dag per gestire le dipendenze dalle tabelle DQM
</pre>
<pre>
Hi Dherraj,
I wanted to ask if the release of the new public key for key_glober is planned for the next release.

Thanks!
</pre>
<pre>
si tratta della tabella con le segnalazioni storiche usata per il blocco degli ndg già segnalati
</pre>
<pre>
Ciao Antonio,
Christian per un problema su monitoring-tool ha chiesto se possiamo verificare se esiste qualche dag o processo su trusted per il caricamento parziale di un file.
Ti ricordi se esiste un dag specifico che fa questa attività (ho chiesto anche a Davide se riesce a varificare)
</pre>
<pre>
<<<<<<< HEAD
ciao Walter,
se vuoi ci sono dalle 11 alle 12
=======
Hi all,
Could someone please approve the following PRs for python-reports and transaction-monitoring, as discussed during yesterday's GCP scope review:

git1

git2

Thanks!
</pre>
<pre>
Hi Sangram,
I've just added the testing subtask. These are from older Jira stories.
>>>>>>> 60e8b6c40ac373744d0b1d2778fa28cfd9f8f566
</pre>
<pre>
ciao Enrico
si tratta di un problema specifico per il modello tax_evasion per quello che riguarda la funzione che crea la colonna alert paese
</pre>
<pre>
Hi Pryanka,
I'm available now.
The problem mainly concerns the deploy action in the Composer repo — it fails at a certain point, and currently it's not possible to deploy an individual branch.
</pre>
<pre>
Here’s the PR — it’s not approved since I only made a few changes for testing in dev.
</pre>
<pre>
Alright, thank you Pryanka. I’ll monitor the workflow to check if it’s functioning properly.
</pre>
<pre>
Okay, thank you Pryanka — it seems to be working now.
</pre>
<pre>
There are several others, but I tested these ones.
</pre>
<pre>
Hi Janani,
I'm currently unable to test it in the dev environment due to an issue with the reporting-engine.jar.
Once the issue is resolved, I'll try submitting it again.
</pre>
<pre>
Hi Janani,
Yes, I'm currently testing the changes in the dev env
</pre>
<pre>
Hi Lavania,

We’ve done an initial check with Ankit and identified the issue in six tables. The problem is related to reading from the dqm_kqi table, which now includes a new column dt_period in the updated schema.

Based on the knowledge transfer (KT), we need to run the following ALTER TABLE statements in production to add this new column:

alter1

alter2

alter3
</pre>
<pre>
The issue is caused by the presence of the SELECT * statement in the KQI query that generates the tables.
</pre>
<pre>
Hi Sangram,
We're unable to reproduce the error in UAT. Could you please provide us with the following Dataproc logs so we can investigate further:

log1

log2

log3

Thank you!

</pre>
<pre>
No, I didn't update any table in UAT except for the PRG one.
I think the issue with data_lake is due to 'bancll2d', which wasn't mentioned in Lavanya's email.
</pre>
<pre>
Hi Sangram,
In the Credito al Consumo log, we found an issue while reading the dwdtdbman1 table. It seems to be related to a corrupted Avro file in the partition for period=20250526.
Could you please check if there’s any problem reading the table for the period 20250526?
</pre>
<pre>
Okay, for the other DAGs, the issue might be resolved by running an ALTER TABLE to add the dt_period column to the tables. Could you please confirm if it's appropriate to open an SR with the CCO_DDSRTE group?
Thanks.
</pre>
<pre>
Could you please run the following ALTER TABLE statements first, and only after that, trigger the DAGs: DAG1, DAG2, and DAG3?
</pre>
<pre>
Hi Surendhar,

I’ve opened SR RIT56456 to trigger the following DAGs:

125479 - trigger1
with these parameters:
{para1, para2}

125479 - tm_oracle
(no parameters)
</pre>
<pre>
Hi Ankit,
I raised SR RITM138338943 on Friday to request changes to the tables.
I’m now waiting for the team to implement the updates.
</pre>
<pre>
I haven’t received the email mentioned, and in order to proceed, the necessary changes to the tables need to be made as indicated in the SR. You could possibly reply in this way.
</pre>
<pre>
Hi Anda,
We’ve made the changes to the public key in production and started sending the files from Friday onward.
Could you please check if they have been received correctly?

Thanks!
Best regards.
</pre>
<pre>
Hi Tarun,
On Friday, the staging tables were re-uploaded to fix the two that were empty.
Regarding the incident, could you please check the dt_business_dt column? SAS is having trouble reading it due to what seems to be an incorrect format.
Can you confirm if the issue originates from Oracle, or if it’s on the SAS side?

Thanks!
</pre>
<pre>
Hi Ashwini,
I believe the instruction should be as shown below.
However, Surendhar is already performing the operation.
Once he’s done, I’ll ask you to clear the failed tasks in Composer.

Thanks!
</pre>
<pre>
Hi Surendhar,
The instructions worked correctly.
Could you please proceed with clearing the following tasks in the DAG related to 125479-4-DG_Only_KQI?

Regarding the task for "credito al consumo", there was an issue with the dwhma table at period=20250526, where it seems a corrupted Avro file is present. This doesn't cause the process to fail, but it may be affecting the results.

Let’s see if the issue occurs again with this new run—if it does, we’ll need to intervene directly on the table.

Thanks!
</pre>
<pre>
Hi Sangram,
We’ve executed the table alteration instructions.
Can we try resubmitting the DAG to check if the issue with the swhmm table has also been resolved?

Thanks!
</pre>
<pre>
Hi Tarun,
On Friday, the staging tables were re-uploaded to fix the two that were empty:

table1

table2

The other tables should not have any data population issues.

Regarding the incident, could you please check the dt_business_dt column? SAS is having trouble reading it due to what seems to be an incorrect format.
Can you confirm whether the issue comes from Oracle, or if it’s on the SAS side?

Thanks!
</pre>
<pre>
Hi Sangram,
We’ve completed the table alteration as instructed.
We can now try resubmitting the DAG to see if the issue with the swhmm table has been resolved as well.

Thanks!
</pre>
<pre>
Hi Ankit,
I think we can reactivate and run the DAG — it might be getting disabled by default in UAT after deployments.
</pre>
<pre>
Good morning everyone,
I wanted to inform you that the monthly run of the Transaction Monitoring DAG was executed today.
At the same time, the Oracle DAG started running a few minutes ago to reload the tables updated with the new month of May.
The Oracle table load is expected to complete in about two hours.
</pre>
<pre>Hi Priyanka,
I've prepared the following PR for the DAG related to Looker logs, including the addition of the looker_slug column.
Could you please review and approve it?
I've also added handling for cases where there are no log rows.</pre>
<pre>
Ciao Laura,
ti inoltro in allegato la nuova estrazione event_driven.
Nell'ultima estrazione automatica non ha estratto righe ne nella parte new ne in quella old in quanto l'estrazione precedente new non aveva estratto nessuna riga.
Nella nuova release nell'old inserirò solo il filtreo a partire dal mese di maggio 2025.

La nuova estrazione allegata contiene la sostituzione della tabella b22 e i filtri sullo stato.
Le colonne sono le seguenti:
- STATO_PRATICA: presenza di pratiche delle categoria indicate nella tabella actual
- Colonne TIPO_PROFILO_NEW a DT_FINE_VALIDITA_NEW sono le colonne della nuova tabella di classificazione

Le righe in rosso sono quelle che verrebbero escluse in relazione allo status mentre gli ndg nel perimetro polizze sono già stati esclusi.
</pre>
<pre>
Hi Hanumantha,

The latest data upload to the staging tables now includes the rows for May 2025. These were missing from the upload on June 6th and were generated by the transaction_monitoring DAG run on June 9th, 2025.

We need to load this new data into the fact tables to keep them aligned with GCP.

Best Regards,
Andrea
</pre>
<pre>
Hi Hanumantha,
Could you please trigger the specified DAGs and pause the one mentioned?
It hasn’t been done yet.
</pre>
<pre>
Hi Anda,
Since the end of last week, after the release, we’ve been sending the GPG and ctrl.csv files to DBExchange.
Could you please confirm if the files are being received and if the encryption key appears to be correct?
Thank you.
</pre>
<pre>
Hi Pryanka,
I briefly mentioned the changes to him in a meeting this morning, but we haven’t discussed them in detail yet. I’ll try to reach out to him by the end of the day.
</pre>
<pre>
Yes, I think I’ll be able to complete the second DAG as well.
</pre>
<pre>
Ciao Antonio,
come ti accennavo ieri con Pryanka attualmente è stato definito un processo di questo tipo:
attualmente nell'ambiente dev di monitoring_tool sono state create le seguenti DAG:
- dag1 per l'estrazione dei log looker da mtool - schedulata giornalmente per le ore 8 PM
- dag2 per l'arricchimento del tracciato con la mail dell'utente sulla base dell'user_id - schedulata giornalmente per le ore 9 PM
Per quanto riguarda questo secondo punto attualmente il file con l'elenco degli utenti mi viene inviato via mail e da quello che ho capito in una prima fase sarà da caricare manualmente in produzione.
Se il processo sarà questo bisognerà capire la fattibilità di tale processo sul progetto di monitoring tool.
</pre>
<pre>
 I've taken a look at the tables you mentioned, and from an initial analysis, it doesn't seem like they're impacting the data_governance queries.
</pre>
<pre>
Ciao Antonio,
ti scrivo in quanto mi ha suggerito Antonio Sannino di chiederti per il progetto monitoring_tool.
Stiamo sviluppando 2 DAG relativi a alla raccolta dei log privacy.
Si tratta di attività stand-alone.
Volevo sapere a chi potevo chiedere per l'approvazione della seguente PR
</pre>
<pre>
Hi Akash, I'm available now if you'd like, or after 14:40 CEST
</pre>
<pre>
Hi Janani,
I don't think I tested it on the reporting-engine DAG, but I did try it on other DAGs where I saw that it doesn't work.
</pre>
<pre>
Ciao Laura,
inoltro in allegato la nuova estrazione basata sui criteri indicati per la settimana dal 09/06/2025 al 1306/2025.
Per quanto riguarda l'ndg questo era presente sulla vecchia estrazione nelle sheet old poi non più estratto nelle successive estrazioni automatiche a seguito di filtri sulle date che non andavano oltre la settimana precedente.
Il filtro per i clienti CIB attualmente non opera sulle pg collegate ma solo sulla segmentazione dell'ndg principale.

Ciao.

</pre>
<pre>
Hi everyone,
I have another call overlapping at 2:00 PM. If possible, could we move today’s meeting to 2:30 PM?
</pre>
<pre>
Hi Satyajit,
Ignore the previous request — Surendar has already approved the PR.
</pre>
<pre>
Ciao Marco,
ti scrivo in quanto Antonio Sannino mi ha suggerito di chiedere a te in relazione al progetto monitoring tool.
Stiamo sviluppando la parte relativa a log privacy e a breve dovremo eseguire il deploy di 2 dag sulla parte composer (PR già inserite).
Volevo chiederti se potevi darmi indicazioni su chi posso contattare eventualmente (lato devops) per questo tipo di attività sul monitoring tool ed eventualmente in futuro per attività su su abilitazioni service-account.

Ciao e Grazie
</pre>
<pre>
------------------------------------
Two new reports are needed, and the end-user specifically asked for them to be delivered as Excel files.

We've set up tests for some code changes we made.

The other changes we made are for existing reports:

DT_Regolamento Report: This was a remapping.
Event-Driven Report: We replaced the bancll2 table with bancllec and banclled.
Sanction Report: We modified the date extraction logic and the transaction query.
------------------------
</pre>
<pre>
Hi Akshay,
I wanted to ask about the daily extract I receive listing the Looker users from the Monitoring Tool project.
Is this extract generated by filtering the Looker project logs to include only users querying BigQuery tables?
</pre>
<pre>
Just to better understand — is the data in the extract retrieved solely by filtering Looker metadata, or is the user ID retrieved from the table we created (with logs and user_id) in the Monitoring Tool project?
</pre>
<pre>
Hi Sangram,
I've added just one report on the integrated hierarchy as requested by Enrico Aimone.
It's a weekly report.
</pre>
<pre>
Hi Janani,
We’ve developed some DAGs to extract logs from the Monitoring Tool project.
The query used to search the logs is in this PR — it filters for queries run by the Looker service account.
The logs only contain the user_id, but not the user_email. To get the email, we need to map it from the Looker project, and we're currently discussing with the Looker team how to do that.
</pre>
<pre>
Assertion 1: Verify that the retrieved rows match the expected ones.
Assertion 2: Verify that the retrieved keywords/rows match the expected keywords/rows.
Assertion 3: Verify that row ID3 is not extracted.
</pre>
<pre>
Sample of rows on which to perform the keyword check.
</pre>
<pre>
The function searches for a single keyword within the text, performing initial preprocessing that includes making the text case-insensitive and removing special characters.
</pre>
<pre>
Hi Hanumantha,
Regarding incident INC24343, I need to verify whether the Oracle tables listed below (checking just one is sufficient) — related to transaction monitoring — contain the column dt_date_business, which the user reports is causing issues in SAS.
This is just to understand whether the issue is limited to the SAS side.

Thank you.
</pre>
<pre>
The user says they are unable to query the table because the field, although it's of type date, actually contains a string in the format YYYY-MM, like 2025-04.
Is it possible to either change the data type to string, or keep it as date but convert the values to a proper date format, for example 2025-04-01?
</pre>
<pre>
Hi Fahad,
I was only trying to check whether Cloud Functions were enabled via the gcloud terminal.
I attempted to run a simple hello_world command using gcloud, but I received an "access denied" error and immediately stopped all testing activities.
It was just a test, but it's clear that this is not the correct way to deploy a Cloud Function.
I’ll make sure to review the proper procedures.
</pre>
<pre>
Thank you, Fadih.
Just to clarify, the command I tried was intended to generate an upload URL in order to deploy the code by uploading it to a temporary Cloud Storage bucket via a signed URL.
However, I believe the deployment should instead take place through the GitHub integration. I'll look into the correct procedure in more detail.

The file I was trying to deploy was just a simple text file with the following lines:
</pre>
<pre>
Hi Hanumantha,
As an alternative, in the next release I will introduce a change in the process of populating the staging tables by modifying the mese_rif field—on which the dt_business_date_dt field is based—so that it contains a string in the correct date format (YYYY-MM-DD), but only for the tables specified by the user. I will first run some tests in UAT.
</pre>
<pre>
We have completed the repopulation of the tables; please proceed with the verification.
As for the malformed field related to incident INC434343, we will implement the necessary changes in the next release.
</pre>
<pre>
Ciao Enrico,
penso che le competenze di base siano quelle relative a:
 - SQL (bigquery)
 - Google cloud environment (gcloud sdk)
 - python
 - Apache Airflow (composer)
 - git (code versioning)
 - Apache Spark (Dataproc)
	- pyspark per lo sviluppo di script python
	- eventualmente scala
Poi ci sono servizi più specifici google come:
 - cloud function
 - dataflow

</pre>
<pre>
Hi Janani,
I was referring to the monitoring tool project's BigQuery, though it might extend to the PWCC project later.


</pre>
<pre>
Yes, I asked who was handling development on MTool, and I was told it's Davide Cui.
I believe Antonio is working on granting the PWCC project's Composer service account access to the MTool logs, so we can proceed with developing the DAG on the PWCC project.
However, I’m not sure how long the process will take to complete.
</pre>
<pre>
Hi all,
This morning, we’re experiencing issues with the following DAGs: the tasks in Airflow are failing despite the corresponding Dataproc jobs having completed successfully.
We would appreciate it if you could look into the issue.
</pre>
<pre>
Details of the comprehensive test:
The test lists were automatically generated by generative AI (Google Gemini) to cover as many keyword search scenarios as possible.

The VASP model includes a simple preprocessing step applied to the columns under validation.
The specific columns vary depending on whether the operations are IN or OUT.
</pre>
<pre>
The tests are integrated into the SDLC deployment pipeline in the Google Cloud DED environment.
</pre>
<pre>
We used Google's generative AI (Gemini) to simulate the bonifici table, on which the keyword search from the official VASP list for transaction monitoring was performed (data.csv file).

To run the test, we asked Gemini to generate the expected result based on the provided requirements.

We applied the transaction monitoring keyword search functions to the simulated bonifici table.

We created unit tests to verify that the functions produced the expected result, as defined in the expected.csv file.
</pre>
<pre>
ciao Enrico,
per Morm di transaction-monitoring ti chiedo se hai una della documentazione da poter fornire per la data-quality su dwh (generico sui dati in origine oltre a quella specifica per TM).
</pre>
<pre>
Could you please mark the Dataproc step as succeeded for the following DAG run dated 20-06-2025?
</pre>
<pre>
Could you please mark the Dataproc step as succeeded for the following DAG run dated 20-06-2025?
The task is currently marked as failed, even though the Dataproc job completed successfully.
For reference, I have opened support request SR 3434343.
</pre>
<pre>
Hi Igor,
The DAG we are currently using is the following:
looker_bigquery.py, which queries the logs of the dev-mtool project.

The DAG you mentioned, which uses the dblooker project, returns a gcloud.logging.read PERMISSION_DENIED error,
whereas the looker_bigquery DAG (file: looker_bq_queries) works correctly.
</pre>
<pre>
Hi Surendhar,
The DAGs ran successfully.
Thank you.
</pre>
<pre>
The lists were generated prior to the test using Gemini's generative AI. Gemini was prompted to create two CSV files for the test.

The prompt provided was:

"Using the official keyword list for insurance, along with exclusion parameters and the search function, create a list of transactions with the expected format for transaction-monitoring transfers. Based on the above requirements, also create a table with the expected results."

This test aims to verify that applying the searchML function accurately extracts the rows present in the extracted.csv file.
</pre>
<pre>
Hi Laraib,
I confirm that the alert file was uploaded on June 9th, 2025.

Best regards.
</pre>
<pre>
The following command had been implemented in the code, which does not attempt any modification but simply retrieves the account's permissions.
However, the command failed and will not be executed again.
Feel free to contact me if you need any further clarification.
</pre>
<pre>
Hi Davide,

Yes, this was a test carried out in the development environment as part of the PG2 log development work.
The gcloud command included in the code was used solely to query the permissions of the sa-composer service account for testing purposes.

The development involves data integration between two separate projects (mtool and dblooker).
The code was designed only to retrieve information, not to perform any modifications. In any case, as shown in the logs, the process returned an error and will not be repeated.

The command used was in fact gcloud projects get-iam-policy, which is strictly for querying assigned roles and permissions.
</pre>
<pre>
Ciao Davide,
esatto si tratta di un test eseguito in dev nell'ambito dello sviluppo dei log PG2 tramite dag composer.
L'istruzione gcloud get-iam-policy inserita nel codice era servita per interrogare i permessi del sa-composer alle sole finalità di test.
Lo sviluppo infatti prevede l'integrazione di dati tra 2 progetti diversi (mtool e dblooker).
Il codice aveva solo finalità di interrogazione e non di modifica in ogni caso il processo (come si evince dai log) ha restituito un errore e non sarà più replicato.

Il comando utilizzato è infatti gcloud project get-iam-policy che ha finalità solo di interrogative (roles and permission assigned)

Cordiali Saluti
</pre>
<pre>
This is to clarify that the gcloud projects get-iam-policy command was used as part of a controlled test conducted in the development environment during the implementation of the PG2 log feature.

The command was embedded within a Composer DAG and was used solely to retrieve the IAM policy associated with the sa-composer service account. Its exclusive purpose was to verify permissions as part of a test scenario — no modifications or write operations were performed.

This step was necessary to support development work involving data integration between two separate GCP projects (mtool and dblooker). Given that cross-project access is involved, it was essential to validate which permissions were already granted to ensure proper handling during future integration.

It's important to note that:

The command in question (gcloud projects get-iam-policy) is strictly read-only.

The process resulted in an error, as confirmed by the logs.

The operation was part of a one-time test and will not be repeated.
</pre>
<pre>
Hi Alina,

On our end, the files are being sent via SFTP to DBExchange, and the process completes successfully for both the counterparty and position files.

Based on the information available to us, there was an open incident concerning the loading of the position file into the folders you mentioned.

For this reason, I’m looping in @Anda Florea and @Christian to see if there are any updates regarding the resolution of this issue.

Below are the logs from the last two transfers to DBExchange from our side:

log1

log2
</pre>
<pre>
For this reason, I’m reaching out to @Anda Florea and @Christian to check if they have any updates on the resolution status of the issue.
</pre>
<pre>
Thank you, Alexandru.

On the GCP side, the files are being sent to the same folder for both the counterparty and position files.
Today's transfer was also completed successfully:

file1

file2

We will go ahead and open an incident with DBExchange to further investigate the issue.
</pre>
<pre>
Missing Files - posion.csv and posi2.csv on DBexchange (dbb.us.db.com)

This email concerns an issue with file transfers to your DBexchange server (dbb.us.db.com).

We regularly send the files posion.csv and posi2.csv to the /globerR destination folder on a daily basis. Despite our logs showing that these transfers are successfully completed, the end-user reports not receiving them.

Our standard delivery time for these files is approximately 7:15 AM (UTC), with retrieval typically occurring around 12:00 PM (UTC).

We kindly request your urgent assistance in investigating why the aforementioned files are not reaching the end-user.
</pre>
<pre>
l’estrazione è attualmente pianificata per la prima settimana di luglio.

Durante le attività di test, non siamo in grado di replicare esattamente l’insieme di NDG selezionati nell’estrazione di aprile, in quanto si tratta di un campionamento casuale. Ogni esecuzione del processo genera, per sua natura, un campione differente.
</pre>
<pre>
DAG triggered upon completion of the monthly DAG 124354-transaction-monitoring.
This process is responsible for loading the individual model tables into the Oracle datamart for use by the AFC team.
</pre>
<pre>
This issue concerns DBExchange, where the end user reports not receiving a specific file, despite regularly receiving another file in the same folder.
Our logs do not show any errors, and the file appears to have been sent correctly.
The end user processes the data at 12 PM UTC.
We need to understand whether the file is being deleted or if, for some reason, it is not reaching its destination.
</pre>
<pre>
Monitoring of the DAGs is carried out according to the standard procedures defined in NAR 125479. It includes sending a summary email to the L2 team multiple times a day, highlighting any failed DAGs and the need to investigate potential technical issues.
Below is an example of the email format used.
</pre>
<pre>
Hi Tarun,
Apologies, but I’m not familiar with that table name
</pre>
<pre>
Subject: Recreated Files

Hi Alina,
I’m sharing the recreated files, built using the same functions as in production.
Just a note—I don’t have access to download from production and would need to ask someone if needed.

Best regards,
Andrea
</pre>
<pre>
**Subject: PySpark AWP Issue – Package Incompatibility on Dataproc Cluster**

We are writing to report a package compatibility issue within the Dataproc cluster.

When running the script, importing the `pandas` package—or other Google Cloud packages such as `bigquery`, which depend on `pandas`—results in the following error:

The issue appears to stem from an incompatibility between the versions of `numpy` and `pandas` currently installed on the cluster:

* `numpy`: 3.2.1
* `pandas`: 1.5.2

We kindly ask if it's possible to resolve this issue.

Thank you.

</pre>
<pre>
To be fixed in the next release on July 3rd, as per the following JIRA task.
</pre>
<pre>
Updates to be made to the process:

Add a function to retrieve the latest file in the bucket folder

Apply the change in both reporting-python and Composer

File data should be taken from the Excel file, not from the DAG
</pre>
<pre>
Hi Pryanka,
Would it be possible to move today's call to after 2:30 PM?

Thanks.
</pre>
<pre>
Hi Alina,
Attached are the CSV files before encryption.
</pre>
<pre>
Hi Sangram,
I've started the checks again. Another approval is needed now — could you please approve it?
</pre>
<pre>
Hi Surendhar,
Could you please re-approve the following PR?
There was an issue with the checks, which I've now fixed.
</pre>
<pre>
The Pull Request is now approved and merged.
The issue was with a template I had set up for some tests in UAT.
</pre>
<pre>
Hi Sangram,
I saw that the DAG related to data_governance failed at the file_exists_check step.
The file arrived in the bucket the following day.
I've opened SR 12564646 to clear the failed task from 05/07/2025.
</pre>
<pre>
Regarding DAG 125479-4-kqi dg, please clear the file_check task so the run can continue.
</pre>
<pre>
Hi Sangram,
I noticed that the DAG related to data_governance failed at the file_exists_check step.
This happened because the file for 2025-07-05 only arrived in the bucket on 2025-07-06.
I've opened SR 12564646 to request clearing the failed task from 05/07/2025
</pre>
<pre>
Hi Sangram,
The DAG failed on `credto_consumo`.
I believe the issue is related to these two tables.
To be sure, could you please share the following Dataproc log:
`1244-job1`?
</pre>
<pre>
I need to ask the L2 team since Sangram doesn't have access to production.
I'll check with Hanumantha right away.
</pre>
<pre>
I'll check why the column isn't being extracted and, if needed, replace it with a null value.
The fix will be applied starting from the next release on 16/07.
</pre>
<pre>
I believe so, but I'll let you know if any issues come up.
I think I'll need to check the CSV file generation step.
</pre>
<pre>
I'm reaching out to the L2 team, as it seems no one currently has access to the production logs.
However, they were supposedly given temporary access and are currently checking.
I believe the issue in production is related to the following two tables:

* tab1
* tab2

These tables likely need the `dt_period` column added, as both have become native tables.

</pre>
<pre>
The issue is related to these two tables: ocs_dwh1 and ocs_dwh2, which now include a new column called dt_period.
The query in the Excel file uses SELECT *, and the tables were modified on 2025-07-05.
</pre>
<pre>
I believe the latest run was on 2025-06-09, while the changes to the tables were made on 2025-06-13.
</pre>
<pre>
Hi Hanumantha,
I've opened this SR to address the issues with DG.
The following DAGs need to be triggered:

* dag1
* dag2

—and only after these have run, then trigger:

* dag3

</pre>
<pre>
Hi Sangram,
The changes to the table won’t fix the problem with the DAG.
Is there any way to get access to the Dataproc logs?
Roberto doesn’t have access either.
</pre>
<pre>
Ciao Alessandro,
ho provato a fare dei controlli e sulla base delle logiche del report ossia l'estrazione del flag_consulenza dal bancll0c non risultano righe in consulenza per il campione di ordini verificati.
Andando sulla tabella target_market delle righe vengono trovate, non ho accesso a FEU e non riesco a verificare se le risultanze della tabella target ma
</pre>
<pre>
Hi Sangram,
I finally checked the logs, and the DG DAG for the current month has now completed successfully.
</pre>
<pre>
Hi Hanumantha,
The DAG has now completed successfully.
</pre>
<pre>
Hi Sangram,
incident INC53453 concerns the loading of the transaction monitoring tables on Oracle.
This afternoon’s Oracle DAG run will include the latest release changes. Once completed, the incident will be moved to “Solved Proposed” status.
</pre>