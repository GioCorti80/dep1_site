<h1>Tasks</h1>
<p>Could you also kindly provide us with the alias to write to the network folder \fdsfs\gfdgdf?</p>

<p>import pandas as pd

    # Sample DataFrame
    data = {'numbers': ['123456', '123456789012345678', '123456789012']}
    df = pd.DataFrame(data)
    
    # Normalize to 12 digits: Pad with zeros if <12 digits, truncate if >12 digits
    df['normalized'] = df['numbers'].apply(lambda x: str(x).zfill(12)[:12])
    
    print(df)
    </p>

<p>pd.set_option('display.max_columns', None)</p>

<p># Rename specific columns using a dictionary
    df_renamed = df.rename(columns={'col1': 'new_col_a', 'col3': 'new_col_c'})
    print("\nDataFrame after renaming specific columns:")
    print(df_renamed)</p>

<pre>from pyspark.sql import SparkSession

    # Create Spark Session with increased memory
    spark = SparkSession.builder \
        .appName("Avoid Heap Memory Error") \
        .config("spark.driver.memory", "4g") \  # Increase driver memory
        .config("spark.executor.memory", "4g") \  # Increase executor memory
        .config("spark.executor.memoryOverhead", "1g") \  # Additional overhead memory
        .config("spark.sql.shuffle.partitions", "200") \  # Reduce shuffle partitions if needed
        .getOrCreate()
    </pre>

    <p>Weâ€™ll have the information only after the CR is completed.</p>

<pre>import importlib
import your_package  # Import the package initially

# Reload the package
importlib.reload(your_package)
</pre>
<pre>
# Rename specific columns
df = df.rename(columns={'old_name1': 'new_name1', 'old_name2': 'new_name2'})
</pre>